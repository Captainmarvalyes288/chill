{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfquery\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"deepseek-r1-distill-qwen-32b\", api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='INTRODUCTION TO DATA SCIENCE \\n1 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nINTRODUCTION TO DATA SCIENCE \\n LECTURE NOTES \\nUNIT - 1 \\nIntroduction to data science \\nData science: \\nData science is the domain of study that deals with vast volumes of data using modern tools and \\ntechniques to find unseen patterns, derive meaningful information, and make business decisions. \\nData science uses complex machine learning algorithms to build predictive models. \\nThe data used for analysis can come from many different sources and presented in various \\nformats. \\nData science is about extraction, preparation, analysis, visualization, and maintenance of \\ninformation. It is a cross disciplinary field which uses scientific methods and processes to draw \\ninsights from data. \\nThe Data Science Lifecycle \\nData science’s lifecycle consists of five distinct stages, each with its own tasks: \\nCapture: Data Acquisition, Data Entry, Signal Reception, Data Extraction. This stage involves \\ngathering raw structured and unstructured data. \\nMaintain: Data Warehousing, Data Cleansing, Data Staging, Data Processing, Data Architecture. \\nThis stage covers taking the raw data and putting it in a form that can be used. \\nProcess: Data Mining, Clustering/Classification, Data Modeling, Data Summarization. Data \\nscientists take the prepared data and examine its patterns, ranges, and biases to determine how \\nuseful it will be in predictive analysis. \\nAnalyze: Exploratory/Confirmatory, Predictive Analysis, Regression, Text Mining, Qualitative \\nAnalysis. Here is the real meat of the lifecycle. This stage involves performing the various \\nanalyses on the data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 1, 'page_label': '2'}, page_content='INTRODUCTION TO DATA SCIENCE \\n2 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nCommunicate: Data Reporting, Data Visualization, Business Intelligence, Decision Making. In \\nthis final step, analysts prepare the analyses in easily readable forms such as charts, graphs, and \\nreports. \\nEvolution of Data Science: Growth & Innovation \\nData science was born from the idea of merging applied statistics with computer science. The \\nresulting field of study would use the extraordinary power of modern computing. Scientists \\nrealized they could not only collect data and solve statistical problems but also use that data to \\nsolve real-world problems and make reliable fact-driven predictions. \\n1962: American mathematician John W. Tukey first articulated the data science dream. In his \\nnow-famous article “The Future of Data Analysis,” he foresaw the inevitable emergence of a new \\nfield nearly two decades before the first personal computers. While Tukey was ahead of his time, \\nhe was not alone in his early appreciation of what would come to be known as “data science.” \\n1977: The theories and predictions of “pre” data scientists like Tukey and Naur became more \\nconcrete with the establishment of The International Association for Statistical Computing \\n(IASC), whose mission was “to link traditional statistical methodology, modern computer \\ntechnology, and the knowledge of domain experts in order to convert data into information and \\nknowledge.” \\n1980s and 1990s: Data science began taking more significant strides with the emergence of the \\nfirst Knowledge Discovery in Databases (KDD) workshop and the founding of the International \\nFederation of Classification Societies (IFCS). \\n1994: Business Week published a story on the new phenomenon of “Database Marketing.” It \\ndescribed the process by which businesses were collecting and leveraging enormous amounts of \\ndata to learn more about their customers, competition, or advertising techniques.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 2, 'page_label': '3'}, page_content='INTRODUCTION TO DATA SCIENCE \\n3 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n1990s and early 2000s: We can clearly see that data science has emerged as a recognized and \\nspecialized field. Several data science academic journals began to circulate, and data science \\nproponents like Jeff Wu and William S. Cleveland continued to help develop and expound upon \\nthe necessity and potential of data science. \\n2000s: Technology made enormous leaps by providing nearly universal access to internet \\nconnectivity, communication, and (of course) data collection. \\n2005: Big data enters the scene. With tech giants such as Google and Facebook uncovering large \\namounts of data, new technologies capable of processing them became necessary. Hadoop rose to \\nthe challenge, and later on Spark and Cassandra made their debuts. \\n2014: Due to the increasing importance of data, and organizations’ interest in finding patterns and \\nmaking better business decisions, demand for data scientists began to see dramatic growth in \\ndifferent parts of the world. \\n2015: Machine learning, deep learning, and Artificial Intelligence (AI) officially enter the realm \\nof data science.  \\n2018: New regulations in the field are perhaps one of the biggest aspects in the evolution in data \\nscience. \\n2020s: We are seeing additional breakthroughs in AI, machine learning, and an ever-more-\\nincreasing demand for qualified professionals in Big Data \\nRoles in Data Science  \\nData Analyst  \\nData Engineers  \\nDatabase Administrator  \\nMachine Learning Engineer'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 3, 'page_label': '4'}, page_content='INTRODUCTION TO DATA SCIENCE \\n4 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nData Scientist  \\nData Architect  \\nStatistician  \\nBusiness Analyst  \\nData and Analytics Manager  \\n1. Data Analyst \\nData analysts are responsib le for a variety of tasks including  visualisation , munging, and \\nprocessing of massive amounts of data. They also have to perform queries on the databases from \\ntime to time. One of the most important skills of a data analyst  is optimization.  \\nFew Important Roles and Responsibilities of a Data Analyst include: \\nExtracting data from primary and secondary sources using automated tools \\nDeveloping and maintaining databases \\nPerforming data analysis and making reports with recommendations \\nTo become a data analyst: SQL, R, SAS, and Python are some of the sought-after technologies for \\ndata analysis. \\n2. Data Engineers \\nData engineers build and test scalable Big Data ecosystems for the businesses so that the  data \\nscientists can run their algorithms on the data systems that are stable and  highly optimized. Data \\nengineers also update the existing systems with newer or upgraded versions of the current \\ntechnologies to improve the efficiency of the databases. \\nFew Important Roles and Responsibilities of a Data Engineer include: \\nDesign and maintain data management systems'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 4, 'page_label': '5'}, page_content='INTRODUCTION TO DATA SCIENCE \\n5 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nData collection/acquisition and management \\nConducting primary and secondary research \\nTo become data engineer: technologies that require hands -on experience include Hive, NoSQL, \\nR, Ruby, Java, C++, and Matlab.  \\n3. Database Administrator \\nThe job profile of a database administrator is pretty much self -explanatory- they are responsible \\nfor the proper functioning of all the databases of an enterprise and grant or revoke its services to \\nthe employees of the company depending on their requirements. \\nFew Important Roles and Responsibilities of a Database Administrator include: \\n\\uf0b7 Working on database software to store and manage data \\n\\uf0b7 Working on database design and development \\n\\uf0b7 Implementing security measures for database \\n\\uf0b7 Preparing reports, documentation, and operating manuals \\nTo become database administrator:  database backup and recovery, data security, data modeling, \\nand design, etc \\n4. Machine Learning Engineer \\nMachine learning engineers  are in high demand today. However, the job profile comes with its \\nchallenges. Apart from having in -depth knowledge of some of the most powerful technologies \\nsuch as SQL, REST APIs, etc.  machine l earning engineers are also expected to perform A/B \\ntesting, build data pipelines, and implement common  machine learning algorithms  such as \\nclassification, clustering, etc. \\nFew Important Roles and Responsibilities of a Machine Learning Engineer include: \\n\\uf0b7 Designing and developing Machine Learning systems \\n\\uf0b7 Researching Machine Learning Algorithms'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 5, 'page_label': '6'}, page_content='INTRODUCTION TO DATA SCIENCE \\n6 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Testing Machine Learning systems \\n\\uf0b7 Developing apps/products basis client requirements \\nTo become machine learning engineer: technologies like Java, Python, JS, etc. Secondly, you \\nshould have a strong grasp of statistics and mathematics.  \\n5. Data Scientist \\nData scientists  have to understand the challenges of business and offer the best solutions using \\ndata analysis and data processing. For instance, they are expected to perform pr edictive analysis \\nand run a fine -toothed comb through an “unstructured/disorganized” data to offer actionable \\ninsights. \\nFew Important Roles and Responsibilities of a Data Scientist include: \\n\\uf0b7 Identifying data collection sources for business needs \\n\\uf0b7 Processing, cleansing, and integrating data \\n\\uf0b7 Automation data collection and management process \\n\\uf0b7 Using Data Science techniques/tools to improve processes \\nTo become a data scientist , you have to be an expert in R, MatLab, SQL, Python, and other \\ncomplementary technologies.  \\n6. Data Architect \\nA data architect  creates the blueprints for data management so that the databases can be easily \\nintegrated, centralized, and protected with the best security measures. They also ensure that the \\ndata engineers have the best tools and systems to work with. \\nFew Important Roles and Responsibilities of a Data Architect include: \\n\\uf0b7 Developing and implementing overall data strategy in line with business/organization \\n\\uf0b7 Identifying data collection sources in line with data strategy \\n\\uf0b7 Collaborating with cross -functional teams and stakeholders for smooth functioning of \\ndatabase systems'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 6, 'page_label': '7'}, page_content='INTRODUCTION TO DATA SCIENCE \\n7 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Planning and managing end-to-end data architecture \\nTo become a data architect: requires expertise in data warehousing, data modelling, extraction \\ntransformation and loan (ETL), etc. You also must be well versed in Hive, Pig, and Spark, etc. \\n7. Statistician \\nA statistician, as the name suggests, has a sound understanding of statistical theories and data \\norganization. Not only do they extract and offer valuable insights from the data clusters, but they \\nalso help create new methodologies for the engineers to apply. \\nFew Important Roles and Responsibilities of a Statistician include: \\n\\uf0b7 Collecting, analyzing, and interpreting data \\n\\uf0b7 Analyzing data, assessing results, and predicting trends/relationships using statistical \\nmethodologies/tools \\n\\uf0b7 Designing data collection processes \\nTo become a statistician: SQL, data mining, and the various machine learning technologies. \\n8. Business Analyst \\nThe role of  business analysts  is slightly different than other  data science jobs . While they do \\nhave a good understanding of how data -oriented technologies work and how to handle large \\nvolumes of data, they also separate the high -value data from the low -value data.   \\nFew Important Roles and Responsibilities of a Business Analyst include: \\n\\uf0b7 Understanding the business of the organization \\n\\uf0b7 Conducting detailed business analysis – outlining problems, opportunities, and solutions \\n\\uf0b7 Working on improving existing business processes \\nTo become business analyst: understand ing of business finances and  business intelligence , and \\nalso the IT technologies like data modelling, data visualization tools, etc.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 7, 'page_label': '8'}, page_content='INTRODUCTION TO DATA SCIENCE \\n8 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nStages in a data science project \\nData Science workflows tend to happen in a wide range of domains and areas of expertise such as \\nbiology, geography, finance or business, among others. This means that Data Science projects can \\ntake on very different challenges and focuses resulting in very  different methods and data sets \\nbeing used. A  Data Science project will have to go through five key stages: defining a problem, \\ndata processing, modelling, evaluation and deployment.  \\nDefining a problem \\n\\uf0b7 The first stage of any Data Science project is to identify and define a problem to be solved. \\nWithout a clearly defined problem to solve, it can be difficult to know how to tackle to the \\nproblem. \\n\\uf0b7 For a Data Science project this can include what method to use, such as is classification, \\nregression or cluste ring. Also, without a clearly defined problem, it can be hard to \\ndetermine what your measure of success would be.  \\n\\uf0b7 Without a defined measure of success, you can never know when your project is complete \\nor is good enough to be used in production.  \\n\\uf0b7 A challen ge with this is being able to define a problem small enough that it can be \\nsolved/tackled individually.  \\nData Processing \\n\\uf0b7 Once you have your problem, how you are going to measure success, and an idea of the \\nmethods you will be using, you can then go about performing the all important task of data \\nprocessing. This is often the stage that will take the longest in any Data Science project \\nand can regularly be the most important stage.  \\n\\uf0b7 There are a variety of tasks that need to occur at this stage depending on what problem \\nyou are going to tackle. The first is often finding ways to create or capture data that \\ndoesn’t exist yet.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 8, 'page_label': '9'}, page_content='INTRODUCTION TO DATA SCIENCE \\n9 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Once you have created this data, you then need to collect it somewhere and in a format \\nthat is useful for your model. This will depend on what method you will be using in the \\nmodelling phase but it will involve figuring out how you will feed the data into your \\nmodel.  \\n\\uf0b7 The final part of this is to then perform any pre -processing steps to ensure that the data is \\nclean enough for the modelli ng method to work. This may involve removing outliers, or \\nchoosing to keep them, manipulating null values, whether a null value is a measure or \\nwhether it should be imputed to the average, or standardising the measures.  \\nModelling \\n\\uf0b7 The next part, and often the most fun and exciting part, is the modelling phase of the Data \\nScience project. The format this will take will depend primarily on what the problem is \\nand how you defined success in the first step, and secondarily on how you processed the \\ndata.  \\n\\uf0b7 Unfortunately, this is often the part that will take the least amount of time of any Data \\nScience project, especially when there are many frameworks or libraries that exist, such as \\nsklearn, statsmodels, tensorflow and that can be readily utilised. \\n\\uf0b7 You should ha ve selected the method that you will be using to model your data in the \\ndefining a problem stage, and this may include simple graphical exploration, regression, \\nclassification or clustering. \\nEvaluation \\n\\uf0b7 Once you have then created and implemented your models, you then need to know how to \\nevaluate it. Again, this goes back to the problem formulation stage where you will have \\ndefined your measure of success, but this is often one of the most important stages. \\n\\uf0b7 Depending on how you processed your data and set -up your model, you may have a \\nholdout dataset or testing data set that can be used to evaluate your model. On this dataset,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 9, 'page_label': '10'}, page_content='INTRODUCTION TO DATA SCIENCE \\n10 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nyou are aiming to see how well your model performs in terms of both accuracy and \\nreliability.  \\nDeployment \\nFinally, once you have robustly evaluated your model and are satisfied with the results, then you \\ncan deploy it into production. This can mean a variety of things such as whether you use the \\ninsights from the model to make changes in your business, whether you use your model to check \\nwhether changes that have been made were successful, or whether the model is deployed \\nsomewhere to continually receive and evaluate live data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 10, 'page_label': '11'}, page_content='INTRODUCTION TO DATA SCIENCE \\n11 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nApplications of data science in various fields \\nMajor Applications of Data Science \\n1. In Search Engines \\nThe most useful application of Data Science is Search Engines. As we know when we want to \\nsearch for something on the internet, we mostly used Search engines like Google, Yahoo, \\nSafari, Firefox, etc. So Data Science is used to get Searches faster.  \\n2. In Transport \\nData Science also entered into the Transport field like Driverless Cars. With the help of \\nDriverless Cars, it is easy to reduce the number of Accidents.  \\nFor Example, In Driverless Cars the training data is fed into the algorithm and with the hel p \\nof Data Science techniques, the Data is analyzed like what is the speed limit in Highway, Busy \\nStreets, Narrow Roads, etc. And how to handle different situations while driving etc.  \\n3. In Finance \\nData Science plays a key role in Financial Industries. Fina ncial Industries always have an issue \\nof fraud and risk of losses. Thus, Financial Industries needs to automate risk of loss analysis in \\norder to carry out strategic decisions for the company. Also, Financial Industries uses Data \\nScience Analytics tools in  order to predict the future.  \\nFor Example, In Stock Market, Data Science is the main part. In the Stock Market, Data \\nScience is used to examine past behavior with past data and their goal is to examine the future \\noutcome. \\n4. In E-Commerce \\nE-Commerce Websites like Amazon, Flipkart, etc. uses data Science to make a better user \\nexperience with personalized recommendations.  \\nFor Example, When we search for something on the E -commerce websites we get suggestions \\nsimilar to choices according to ou r past data and also we get recommendations according to \\nmost buy the product, most rated, most searched, etc. This is all done with the help of Data \\nScience. \\n5. In Health Care \\nIn the Healthcare Industry data science act as a boon. Data Science is used for : \\n\\uf0b7 Detecting Tumor.  \\n\\uf0b7 Drug discoveries.  \\n\\uf0b7 Medical Image Analysis.  \\n\\uf0b7 Virtual Medical Bots.  \\n\\uf0b7 Genetics and Genomics.  \\n\\uf0b7 Predictive Modeling for Diagnosis etc.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 11, 'page_label': '12'}, page_content='INTRODUCTION TO DATA SCIENCE \\n12 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n6. Image Recognition \\nCurrently, Data Science is also used in Image Recognition.  For Example, When we upload \\nour image with our friend on Facebook, Facebook gives suggestions Tagging who is in the \\npicture. This is done with the help of machine learning and Data Science. When an Image is \\nRecognized, the data analysis is done on one’s Facebook friends and after analys is, if the faces \\nwhich are present in the picture matched with someone else profile then Facebook suggests us \\nauto-tagging.   \\n7. Targeting Recommendation \\nTargeting Recommendation is the most important application of Data Science. Whatever the \\nuser searches on the Internet, he/she will see numerous posts everywhere.  \\nexample: Suppose I want a mobile phone, so I just Google search it and after that, I changed  \\nmy mind to buy offline. Data Science helps those companies who are paying for \\nAdvertisements for their mobile. So everywhere on the internet in the social media, in the \\nwebsites, in the apps everywhere I will see the recommendation of that mobile phone wh ich I \\nsearched for. So this will force me to buy online.  \\n8. Airline Routing Planning \\nWith the help of Data Science, Airline Sector is also growing like with the help of it, it \\nbecomes easy to predict flight delays. It also helps to decide whether to direct ly land into the \\ndestination or take a halt in between like a flight can have a direct route from Delhi to the \\nU.S.A or it can halt in between after that reach at the destination.  \\n9. Data Science in Gaming \\nIn most of the games where a user will play with a n opponent i.e. a Computer Opponent, data \\nscience concepts are used with machine learning where with the help of past data the Computer \\nwill improve its performance. There are many games like Chess, EA Sports, etc. will use Data \\nScience concepts.  \\n10. Medicine and Drug Development \\nThe process of creating medicine is very difficult and time -consuming and has to be done with \\nfull disciplined because it is a matter of Someone’s life. Without Data Science, it takes lots of \\ntime, resources, and finance or develop ing new Medicine or drug but with the help of Data \\nScience, it becomes easy because the prediction of success rate can be easily determined based \\non biological data or factors. The algorithms based on data science will forecast how this will \\nreact to the h uman body without lab experiments.  \\n11. In Delivery Logistics \\nVarious Logistics companies like DHL, FedEx, etc. make use of Data Science. Data Science \\nhelps these companies to find the best route for the Shipment of their Products, the best time \\nsuited for delivery, the best mode of transport to reach the destination, etc.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 12, 'page_label': '13'}, page_content='INTRODUCTION TO DATA SCIENCE \\n13 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n12. Autocomplete \\nAutoComplete feature is an important part of Data Science where the user will get the facility \\nto just type a few letters or words, and he will get the feature of auto -completing the line. In \\nGoogle Mail, when we are writing formal mail to someone so at that time data science concept \\nof Autocomplete feature is used where he/she is an efficient choice to auto -complete the whole \\nline.  Also in Search Engines in social media, in various apps, AutoComplete feature is widely \\nused. \\nData security issues  \\nWhat is Data Security? \\nData security is the process of protecting corporate data and preventing data loss through \\nunauthorized access. This includes protecting your data from attacks that can encrypt or destroy \\ndata, such as ransomware, as well as attacks that can modify or corrupt your data. Data security \\nalso ensures data is available to anyone in the organization who has access to it. \\nSome industries require a high level of data security to comply with  data protection regulations. \\nFor example, organizations that process payme nt card information must use and store payment \\ncard data securely, and healthcare organizations in the USA must secure private health \\ninformation (PHI) in line with the HIPAA standard. \\nData Security vs Data Privacy \\nData privacy is the distinction between data in a computer system that can be shared with third \\nparties (non-private data), and data that cannot be shared with third parties (private data). There \\nare two main aspects to enforcing data privacy: \\n\\uf0b7 Access control—ensuring that anyone who tries to access the data is authenticated to confirm \\ntheir identity, and authorized to access only the data they are allowed to access. \\n\\uf0b7 Data protection —ensuring that even if unauthorized pa rties manage to access the data, they \\ncannot view it or cause damage to it. Data protection methods ensure encryption, which prevents \\nanyone from viewing data if they do not have a private encryption key, and data loss prevention \\nmechanisms which prevent users from transferring sensitive data outside the organization. \\nData security has many overlaps with data privacy. The same mechanisms used to ensure data \\nprivacy are also part of an organization’s data security strategy. \\nThe primary difference is that dat a privacy mainly focuses on keeping data confidential, while \\ndata security mainly focuses on protecting from malicious activity.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 13, 'page_label': '14'}, page_content='INTRODUCTION TO DATA SCIENCE \\n14 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nData Security Risks \\n\\uf076 Accidental Exposure \\nA large percentage of data breaches are not the result of a malicious attack but are caused by \\nnegligent or accidental exposure of  sensitive data. It is common for an organization’s employees to \\nshare, grant access to, lose, or mishandle valuable data, either by accident or because they are not \\naware of security policies. \\n \\n\\uf076 Phishing and Other Social Engineering Attacks \\nSocial engineering attacks are a primary vector used by attackers to access sensitive data. \\nThey involve manipulating or tricking individuals into providing private information or access to \\nprivileged accounts. \\nPhishing is a common form of social engineering. It involves messages that appear to be from a \\ntrusted source, but in fact are sent by an attacker.  \\n\\uf076 Insider Threats \\nInsider threats are employees who inadvertently or intentionally threaten the security of an \\norganization’s data. There are three types of insider threats: \\n\\uf0b7 Non-malicious insider —these are users that can cause harm accidentally, via negligence, or \\nbecause they are unaware of security procedures. \\n\\uf0b7 Malicious insider —these are users who actively attempt to steal data or cause harm to the \\norganization for personal gain. \\n\\uf0b7 Compromised insider—these are users who are not aware that their accounts or credentials were \\ncompromised by an external attacker. The attacker can then perform malicious activity, \\npretending to be a legitimate user. \\n\\uf076 Ransomware \\nRansomware is a major threat to data in companies of all sizes. Ransomware is  malware that \\ninfects corporate devices and encrypts data, making it useless without the decryption key.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 14, 'page_label': '15'}, page_content='INTRODUCTION TO DATA SCIENCE \\n15 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nAttackers display a ransom message asking for payment to release the key, but in many cases, \\neven paying the ransom is ineffective and the data is lost. \\n\\uf076 Data Loss in the Cloud \\nMany organizations are moving data to the cloud to facilitate easier sharing and collaboration. \\nHowever, when data moves to the cloud, it is more difficult to control and prevent data loss. Users \\naccess data from personal devices and over  unsecured networks. It is all too easy to share a file \\nwith unauthorized parties, either accidentally or maliciously. \\n\\uf076 SQL Injection \\nSQL injection (SQLi) is a common technique used by attackers to gain illicit access to \\ndatabases, steal data, and perform unwanted operations. It works by adding malicious code to a \\nseemingly innocent database query. \\nCommon Data Security Solutions and Techniques: \\nData Discovery and Classification \\n\\uf0b7 Modern IT environments store data on servers, endpoints, and cloud systems. Visibility \\nover data flows is an important first step in understanding what data is at risk of being \\nstolen or misused. \\n\\uf0b7 To properly protect your data, you need to know the type of data, where it is, and what it is \\nused for. Data discovery and classification tools can help. \\n\\uf0b7 Data detection is the basis for knowing what data you have. Data classification allows you \\nto create scalable security solutions, by identifying which data is sensitive and needs to be \\nsecured. \\nData Masking \\n\\uf0b7 Data masking lets you create a synthetic version of your organizational data, which you \\ncan use for software testing, training, and other purposes that don’t require the real data.  \\n\\uf0b7 The goal is to protect data while providing a functional alternative when needed.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 15, 'page_label': '16'}, page_content='INTRODUCTION TO DATA SCIENCE \\n16 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nData Encryption \\n\\uf0b7 Data encryption is a method of converting data from a readable format (plaintext) to an \\nunreadable encoded format (ciphertext). Only after decrypting the encrypted data using the \\ndecryption key, the data can be read or processed. \\n\\uf0b7 In public-key cryptography techniques, there is no need to share the decryption key – the \\nsender and recipient each have their own key, which are combined to perform the \\nencryption operation. This is inherently more secure. \\n\\uf0b7 Data encryption can prevent hackers from accessing sensitive information.  \\nPassword Hygiene \\n\\uf0b7 One of the simplest best practices for data security is ensuring users have unique, strong \\npasswords. Without central management and enforcement, many users will use easily \\nguessable passwords or use the same password for many different services.  \\n\\uf0b7 Password spraying and other brute force attacks can easily compromise accounts with \\nweak passwords. \\nAuthentication and Authorization \\nOrganizations must put in place strong authentication methods, such as OAuth for web-based \\nsystems. It is highly recommended to enforce multi-factor authentication when any user, whether \\ninternal or external, requests sensitive or personal data.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 16, 'page_label': '17'}, page_content='INTRODUCTION TO DATA SCIENCE \\n17 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nUNIT –II \\nDATA COLLECTION AND PREPROCESSING \\nDATA COLLECTION: \\nData collection  is the process of collecting, measuring and analyzing different types of \\ninformation using a set of standard validated techniques. The main objective of data collection is \\nto gather information-rich and reliable data, and analyze them to make critical business decisions. \\nOnce the data is collected, it goes through a rigorous process of  data cleaning  and data \\nprocessing to make this data truly useful for businesses.  \\nThere are two main methods of data collection in research based on the information that is \\nrequired, namely: \\n\\uf0b7 Primary Data Collection \\n\\uf0b7 Secondary Data Collection \\nPrimary Data Collection Methods \\nPrimary data refers to data collected from first -hand experience directly from the main source. It \\nrefers to data that has never been used in the past. The data gathered by primary data collection \\nmethods are generally regarded as the best kind of data in research. \\n\\uf0b7 The methods of collecting primary data can be further divided into quantitative data \\ncollection methods (deals with factors that can be counted) and qualitative data collection \\nmethods (deals with factors that are not necessarily numerical in nature). \\nHere are some of the most common primary data collection methods: \\n1. Interviews \\nInterviews are a direct method of data coll ection. It is simply a process in which the interviewer \\nasks questions and the interviewee responds to them. It provides a high degree of flexibility \\nbecause questions can be adjusted and changed anytime according to the situation.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 17, 'page_label': '18'}, page_content='INTRODUCTION TO DATA SCIENCE \\n18 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n2. Observations \\nIn this method, researchers observe a situation around them and record the findings. It can be used \\nto evaluate the behaviour of different people in controlled (everyone knows they are being \\nobserved) and uncontrolled (no one knows they are being observed) situations. \\n3. Surveys and Questionnaires \\nSurveys and questionnaires provide a broad perspective from large groups of people. They can be \\nconducted face-to-face, mailed, or even posted on the Internet to get respondents from anywhere \\nin the world.  \\n4. Focus Groups \\nA focus group is similar to an interview, but it is conducted with a group of people who all have \\nsomething in common. The data collected is similar to in-person interviews, but they offer a better \\nunderstanding of why a certain group of people thinks in a particular way.  \\n5. Oral Histories \\nOral histories also involve asking questions like interviews and focus groups. However, it is \\ndefined more precisely and the data collected is linked to a single phenomenon. It involves \\ncollecting the opinio ns and personal experiences of people in a particular event that they were \\ninvolved in. \\nSecondary Data Collection Methods \\nSecondary data refers to data that has already been collected by someone else. It is much more \\ninexpensive and easier to collect than primary data. \\nHere are some of the most common secondary data collection methods:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 18, 'page_label': '19'}, page_content='INTRODUCTION TO DATA SCIENCE \\n19 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n1. Internet \\nThe use of the Internet has become one of the most popular secondary data collection methods in \\nrecent times. There is a large pool of free and paid research reso urces that can be easily accessed \\non the Internet.  \\n2. Government Archives \\nThere is lots of data available from government archives that you can make use of. The most \\nimportant advantage is that the data in government archives are authentic and verifiable.  The \\nchallenge, however, is that data is not always readily available due to a number of factors.  \\n3. Libraries \\nMost researchers donate several copies of their academic research to libraries. You can collect \\nimportant and authentic information based on different research contexts.  \\nData preprocessing \\nData preprocessing, a component of data preparation, describes any type of processing performed \\non raw data to prepare it for another data processing procedure. \\nData preprocessing transforms the data into a format that is more easily and effectively processed \\nin data mining, machine learning and other data science tasks. The techniques are generally used \\nat the earliest stages of the  machine learning and AI development pipeline to ensur e accurate \\nresults. \\nThere are several different tools and methods used for preprocessing data, including the \\nfollowing: \\n\\uf0b7 sampling, which selects a representative subset from a large population of data; \\n\\uf0b7 transformation, which manipulates raw data to produce a single input; \\n\\uf0b7 denoising, which removes noise from data; \\n\\uf0b7 imputation, which synthesizes statistically relevant data for missing values;'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 19, 'page_label': '20'}, page_content='INTRODUCTION TO DATA SCIENCE \\n20 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 normalization, which organizes data for more efficient access; and \\n\\uf0b7 feature extraction, which pulls out a relevant feature subset that is significant in a particular \\ncontext. \\nkey steps in data preprocessing \\n1.  Data profiling . Data profiling is the process of examining, analyzing and reviewing data to \\ncollect statistics about its quality. It starts with a survey of existing data and its characteristics . \\nData scientists identify data sets that are pertinent to the problem at hand, inventory its significant \\nattributes, and form a hypothesis of features that might be relevant for the proposed analytics or \\nmachine learning task.  \\n2. Data cleansing . The aim here is to find the easiest way to rectify quality issues, such as \\neliminating bad data, filling in missing data or otherwise ensuring the raw data is suitable for \\nfeature engineering.  \\n3. Data reduction.  Raw data sets often include redundant data that ari se from characterizing \\nphenomena in different ways or data that is not relevant to a particular ML, AI or analytics task. \\nData reduction uses techniques like principal component analysis to transform the raw data into a \\nsimpler form suitable for particular use cases. \\n4. Data transformation. Here, data scientists think about how different aspects of the data need \\nto be organized to make the most sense for the goal. This could include things like \\nstructuring unstructured data , combining salient variables when it makes sense or identifying \\nimportant ranges to focus on. \\n5. Data enrichment. In this step, data scientists apply the various feature engineering libraries to \\nthe data to effect the desired transformations. The result should be a data set organized to achieve \\nthe optimal balance between the training time for a new model and the required compute. \\n6. Data validation . At this stage, the data is split into two sets. The first set is used to train a \\nmachine learning or deep learning model. The second set is the testing data that is used to gauge \\nthe accuracy and robustness of the resulting model.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 20, 'page_label': '21'}, page_content='INTRODUCTION TO DATA SCIENCE \\n21 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nData Preprocessing in Data Mining  \\nData preprocessing is the process of transforming raw data into an understandable format. It is \\nalso an important step in data mining as we cannot work with raw data. The quality of the data \\nshould be checked before applying machine learning or data mining algorithms. \\nMajor Tasks in Data Preprocessing: \\n1. Data cleaning \\n2. Data integration \\n3. Data reduction \\n4. Data transformation'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 21, 'page_label': '22'}, page_content='INTRODUCTION TO DATA SCIENCE \\n22 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nData Preprocessing in machine learning: \\nMachine Learning ProcessSteps in Data Preprocessing \\n\\uf0b7 Step 1 : Import the libraries \\n\\uf0b7 Step 2 : Import the data-set \\n\\uf0b7 Step 3 : Check out the missing values \\n\\uf0b7 Step 4 : See the Categorical Values \\n\\uf0b7 Step 5 : Splitting the data-set into Training and Test Set \\n\\uf0b7 Step 6 : Feature Scaling'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 22, 'page_label': '23'}, page_content='INTRODUCTION TO DATA SCIENCE \\n23 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nStep 1 : Import the Libraries \\nThis is how we import libraries in Python using import keyword and this is the most popular \\nlibraries which any Data Scientist used. \\nNumPy is the fundamental package for scientific computing with Python. It contains among other \\nthings: \\n1. A powerful N-dimensional array object \\n2. Sophisticated (broadcasting) functions \\n3. Tools for integrating C/C++ and FORTRAN code \\n4. Useful linear algebra, Fourier transform, and random number capabilities \\nPandas is for data manipulation and analysis. Pandas is an open source, BSD-licensed library \\nproviding high-performance, easy-to-use data structures and data analysis tools for \\nthe Python programming language. \\nMatplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, \\nweb application servers, and four graphical user interface toolkits.Seaborn is a Python data \\nvisualization library based on matplotlib.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 23, 'page_label': '24'}, page_content='INTRODUCTION TO DATA SCIENCE \\n24 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nStep 2 : Import the Dataset \\nBy using Pandas we import our data-set and the file I used here is .csv file [Note: It’s not \\nnecessarily every-time you deal with CSV file, sometimes you deal with Html or Xlsx(Excel \\nfile) ]. \\nStep 3 : Check out the Missing Values \\nThe concept of missing values is important to understand in order to successfully manage data. If \\nthe missing values are not handled properly by the researcher, then he/she may end up drawing an \\ninaccurate inference about the data. Due to improper handling, the result obtained by the \\nresearcher will differ from ones where the missing values are present. \\n \\nTwo ways to handle Missing Values in Data Preprocessing \\nThis data preprocessing method is commonly used to handle the null values. \\nDrop the Missing Values \\n \\nThis strategy can be applied on a feature which has numeric data like the year column or Home \\nteam goal column. We can calculate the mean, median or mode of the feature and replace it with \\nthe missing values.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 24, 'page_label': '25'}, page_content='INTRODUCTION TO DATA SCIENCE \\n25 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nReplace the Missing Value \\n \\nStep 4 : See the Categorical Values \\nUse LabelEncoder class to convert Categorical data into numerical one \\nlabel_encoder is object which is I use and help us in transferring Categorical data into Numerical \\ndata. Next, I fitted this label_encoder object to the first column of our matrix X and all this return \\nthe first column country of the matrix X encoded.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 25, 'page_label': '26'}, page_content='INTRODUCTION TO DATA SCIENCE \\n26 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nDummy Variables is one that takes the value 0 or 1 to indicate the absence or presence of some \\ncategorical effect that may be expected to shift the outcome. \\nUsing Pandas to create Dummy Variables \\n \\nStep 5 : Splitting the data-set into Training and Test Set \\nIn any Machine Learning model is that we’re going to split data-set into two separate sets \\n1. Training Set \\n2. Test Set \\nWhy we need splitting ? \\nWell here it’s your algorithm model that is going to learn from your data to make predictions. \\nGenerally we split the data-set into 70:30 ratio or 80:20 what does it mean, 70 percent data take in \\ntrain and 30 percent data take in test. However, this Splitting can be varies according to the data-\\nset shape and size. \\nStep 6 : Feature Scaling \\nFeature scaling is the method to limit the range of variables so that they can be compared on \\ncommon grounds. \\nData cleaning \\nData cleaning is fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or \\nincomplete data within a dataset. If data is incorrect, outcomes and algorithms are unreliable, even'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 26, 'page_label': '27'}, page_content='INTRODUCTION TO DATA SCIENCE \\n27 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nthough they may look correct. When combining multiple data sources, there are many \\nopportunities for data to be duplicated or mislabeled. \\nGenerally, data cleaning reduces errors and improves data quality. Correcting errors in data and \\neliminating bad records can be a time-consuming and tedious process, but it cannot be ignored. \\nSteps of Data Cleaning \\n1. Remove duplicate or irrelevant observations \\nRemove unwanted observations from your dataset, including duplicate observations or irrelevant \\nobservations. Duplicate observations will happen mos t often during data collection. When you \\ncombine data sets from multiple places, scrape data, or receive data from clients or multiple \\ndepartments, there are opportunities to create duplicate data. De -duplication is one of the largest \\nareas to be considered in this process.  \\n2. Fix structural errors \\nStructural errors are when you measure or transfer data and notice strange naming conventions, \\ntypos, or incorrect capitalization. These inconsistencies can cause mislabeled categories or \\nclasses. For example, you may find \"N/A\" and \"Not Applicable\" in any sheet, but they should be \\nanalyzed in the same category. \\n3. Filter unwanted outliers \\nOften, there will be one -off observations where, at a glance, they do not appear to fit within \\nthe data you are analyzing. If you have a legitimate reason to remove an outlier, like improper \\ndata entry, doing so will help the performance of the data you are working with. \\n4. Handle missing data \\nYou can\\'t ignore missing data because many algorithms will not accept missing values. There are \\na couple of ways to deal with missing data. Neither is optimal, but both can be considered, such \\nas: \\no You can drop observations with missing values, but this will drop or lose information, so \\nbe careful before removing it.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 27, 'page_label': '28'}, page_content='INTRODUCTION TO DATA SCIENCE \\n28 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\no You can input missing val ues based on other observations; again, there is an opportunity \\nto lose the integrity of the data because you may be operating from assumptions and not \\nactual observations. \\no You might alter how the data is used to navigate null values effectively. \\n5. Validate and QA  \\nAt the end of the data cleaning process, you should be able to answer these questions as a part of \\nbasic validation, such as: \\no Does the data make sense? \\no Does the data follow the appropriate rules for its field? \\no Does it prove or disprove your working theory or bring any insight to light? \\no Can you find trends in the data to help you for your next theory? \\nMethods of Data Cleaning \\nThere are many data cleaning methods through which the data should be run. The methods are \\ndescribed below: \\n1. Ignore the tuples: This method is not very feasible, as it only comes to use when the \\ntuple has several attributes is has missing values. \\n2. Fill the missing value:  This approach is also not very effective or feasible. Moreover, it \\ncan be a time -consuming method. In the  approach, one has to fill in the missing value. \\nThis is usually done manually, but it can also be done by attribute mean or using the most \\nprobable value. \\n3. Binning method: This approach is very simple to understand. The smoothing of sorted \\ndata is done using the values around it. The data is then divided into several segments of \\nequal size. After that, the different methods are executed to complete the task. \\n4. Regression: The data is made smooth with the help of using the regression function. The \\nregression can be linear or multiple. Linear regression has only one independent variable, \\nand multiple regressions have more than one independent variable. \\n5. Clustering: This method mainly operates on the group. Clustering groups the data in a \\ncluster. Then, the outliers are detected with the help of clustering. Next, the similar values \\nare then arranged into a \"group\" or a \"cluster\".'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 28, 'page_label': '29'}, page_content='INTRODUCTION TO DATA SCIENCE \\n29 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nProcess of Data Cleaning \\nThe following steps show the process of data cleaning in data mining. \\n1. Monitoring the errors: Keep a note of s uitability where the most mistakes arise. It will \\nmake it easier to determine and stabilize false or corrupt information. Information is \\nespecially necessary while integrating another possible alternative with established \\nmanagement software. \\n2. Standardize the mining process: Standardize the point of insertion to assist and reduce \\nthe chances of duplicity. \\n3. Validate data accuracy: Analyze and invest in data tools to clean the record in real-time. \\nTools used Artificial Intelligence to better examine for correctness. \\n4. Scrub for duplicate data:  Determine duplicates to save time when analyzing data. \\nFrequently attempted the same data can be avoided by analyzing and investing in separate \\ndata erasing tools that can analyze rough data in quantity and automate the operation. \\n5. Research on data:  Before this activity, our data must be standardized, validated, and \\nscrubbed for duplicates. There are many third -party sources, and these Approved & \\nauthorized parties sources can capture information directly from our databases. They help \\nus to clean and compile the data to ensure completeness, accuracy, and reliability for \\nbusiness decision-making. \\n6. Communicate with the team: Keeping the group in the loop will assist in developing and \\nstrengthening the client and sending more targeted data to prospective customers. \\n \\nTools for Data Cleaning in Data Mining \\n1. OpenRefine \\n2. Trifacta Wrangler \\n3. Drake \\n4. Data Ladder \\n5. Data Cleaner \\n6. Cloudingo \\n7. Reifier'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 29, 'page_label': '30'}, page_content='INTRODUCTION TO DATA SCIENCE \\n30 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n8. IBM Infosphere Quality Stage \\n9. TIBCO Clarity \\n10. Winpure \\nData Integration \\nData Integration is a data preprocessing technique that combines data from multiple \\nheterogeneous data sources into a coherent data store and provides a unified view of the data. \\nThese sources may include multiple data cubes, databases, or flat files.   \\n \\nThe data integration approach es are formally defined as triple <G, S, M> where,   \\nG stand for the global schema,   \\nS stands for the heterogeneous source of schema,   \\nM stands for mapping between the queries of source and global schema.   \\n \\nThere are mainly 2 major approaches for data inte gration – one is the “tight coupling \\napproach” and another is the “loose coupling approach”.   \\nTight Coupling:   \\n\\uf0b7 Here, a data warehouse is treated as an information retrieval component.  \\n\\uf0b7 In this coupling, data is combined from different sources into a single physical location \\nthrough the process of ETL – Extraction, Transformation, and Loading.  \\nLoose Coupling:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 30, 'page_label': '31'}, page_content='INTRODUCTION TO DATA SCIENCE \\n31 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Here, an interface is provided that takes the query from the user, transforms it in a way the \\nsource database can understand, and then sends the query  directly to the source databases \\nto obtain the result.  \\n\\uf0b7 And the data only remains in the actual source databases.  \\n \\nIssues in Data Integration:   \\nThere are three issues to consider during data integration: Schema Integration, Redundancy \\nDetection, and resolu tion of data value conflicts. These are explained in brief below.   \\n1. Schema Integration:   \\n\\uf0b7 Integrate metadata from different sources.  \\n\\uf0b7 The real -world entities from multiple sources are referred to as the entity identification \\nproblem. \\n2. Redundancy:   \\n\\uf0b7 An attribute may be redundant if it can be derived or obtained from another attribute or set \\nof attributes. \\n\\uf0b7 Inconsistencies in attributes can also cause redundancies in the resulting data set.  \\n\\uf0b7 Some redundancies can be detected by correlation analysis.  \\n3. Detection and resolution of  data value conflicts:  \\n\\uf0b7 This is the third critical issue in data integration.  \\n\\uf0b7 Attribute values from different sources may differ for the same real -world entity. \\n\\uf0b7 An attribute in one system may be recorded at a lower level of abstraction than the “same” \\nattribute in another.  \\nData Transformation \\nData transformation is a technique used to convert the raw data into a suitable format that \\nefficiently eases data mining and retrieves strategic information. Data transformation includes \\ndata cleaning techniques and a data reduction technique to convert the data into the appropriate \\nform. Data transformation changes the format, structure, or values of the data and converts them \\ninto clean, usable data \\n. Data may be transformed at two stages of the data pipeline for data analytics projects. \\nOrganizations that use on-premises data warehouses generally use an ETL (extract, transform, and \\nload) process, in which data transformation is the middle step.  \\nData Transformation Techniques \\n1. Data smoothing \\n2. Attribute construction'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 31, 'page_label': '32'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n32 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n3. Data aggregation \\n4. Data normalization \\n5. Data discretization \\n6. Data generalization \\n1. Data Smoothing \\nData smoothing is a process that is used to remove noise from the dataset using some algorithms. \\nWe have seen how the noise is removed from the data using the techniques such as binning, \\nregression, clustering. \\no Binning: This method splits the sorted data into the number of bins and smoothens the \\ndata values in each bin considering the neighborhood values around it. \\no Regression: This method identifies the relation among two dependent attributes so that if \\nwe have one attribute, it can be used to predict the other attribute. \\no Clustering: This method groups similar data values and form a cluster. The values that lie \\noutside a cluster are known as outliers. \\n2. Attribute Construction \\no In the attribute construction method, the new attributes consult the existing attributes to \\nconstruct a new data set that eases data mining. New attributes are created and applied to \\nassist the mining process from the given attributes. This simplifies the original data and \\nmakes the mining more efficient. \\nEx: we may have the height and width of e ach plot. So here, we can construct a new attribute \\n'area' from attributes 'height' and 'weight'.  \\n3. Data Aggregation \\nData collection or aggregation is the method of storing and presenting data in a summary format. \\nThe data may be obtained from multiple d ata sources to integrate these data sources into a data \\nanalysis description.  \\nFor example, we have a data set of sales reports of an enterprise that has quarterly sales of each \\nyear. We can aggregate the data to get the enterprise's annual sales report.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 32, 'page_label': '33'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n33 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\n4. Data Normalization \\nNormalizing the data refers to scaling the data values to a much smaller range such as [ -1, 1] or \\n[0.0, 1.0]. There are different methods to normalize the data, as discussed below. \\nConsider that we have a numeric attribute A and we h ave n number of observed values for \\nattribute A that are V1, V2, V3, ….Vn. \\nMin-max normalization: This method implements a linear transformation on the original data. \\nLet us consider that we have min A and maxA as the minimum and maximum value observed for \\nattribute A and V iis the value for attribute A that has to be normalized.  \\nThe min -max normalization would map V i to the V' i in a new smaller range [new_min A, \\nnew_maxA]. The formula for min -max normalization is given below:  \\n \\nFor example, we have $1200 and $9800 as the minimum, and maximum value for the attribute \\nincome, and [0.0, 1.0] is the range in which we have to map a value of $73,600.  \\nThe value $73,600 would be transformed using min-max normalization as follows: \\n \\nZ-score normalization : This method normalizes the value for attribute A using \\nthe meanand standard deviation . The following formula is used for Z -score normalization:  \\n \\nHere Ᾱ and σ A are the mean and standard deviation for attribute A, respectively.  \\nFor example, we have a mean and standard deviation for attribute A as $54,000 and $16,000. And \\nwe have to normalize the value $73,600 using z -score normalization.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 33, 'page_label': '34'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n34 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nDecimal Scaling: This method normalizes the value of attribute A by moving the decimal point \\nin the value. This movement of a decima l point depends on the maximum absolute value of A. \\nThe formula for the decimal scaling is given below:  \\n \\nHere j is the smallest integer such that max(| v'i|)<1 \\nFor example, the observed values for attribute A range from -986 to 917, and the maximum \\nabsolute value for attribute A is 986. Here, to normalize each value of attribute A using decimal \\nscaling, we have to divide each value of attribute A by 1000, i.e., j=3.  \\nSo, the value -986 would be normalized to -0.986, and 917 would be normalized to 0.917. \\n5. Data Discretization \\nThis is a process of converting continuous data into a set of data intervals. Continuous attribute \\nvalues are substituted by small interval labels. This makes the data easier to study and analyze.  \\nData discretization can be class ified into two types:  supervised discretization, where the class \\ninformation is used, and  unsupervised discretization, which is based on which direction the \\nprocess proceeds, i.e., 'top-down splitting strategy' or 'bottom-up merging strategy'. \\nFor example, the values for the age attribute can be replaced by the interval labels such as (0 -10, \\n11-20…) or (kid, youth, adult, senior). \\n6. Data Generalization \\nIt converts low -level data attributes to high -level data attributes using concept hierarchy. This \\nconversion from a lower level to a higher conceptual level is useful to get a clearer picture of the \\ndata. Data generalization can be divided into two approaches: \\no Data cube process (OLAP) approach. \\no Attribute-oriented induction (AOI) approach. \\nFor example, age data can be in the form of (20, 30) in a dataset. It is transformed into a higher \\nconceptual level into a categorical value (young, old). \\nData Reduction \\nData reduction techniques ensure the integrity of data while reducing the data. Data reduction is a \\nprocess that reduces the volume of original data and represents it in a much smaller volume.  \\nTechniques of Data Reduction\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 34, 'page_label': '35'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n35 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n1. Dimensionality reduction \\n2. Numerosity reduction \\n3. Data cube aggregation \\n4. Data compression \\n5. Discritization operation \\n1. Dimensionality Reduction \\nWhenever we encounter weakly important data, we use the attribute required for our analysis. \\nDimensionality reduction eliminates the attributes from the data set under consideration, \\nthereby reducing the volume of original data. \\nHere are three methods of dimensionality reduction. \\n\\uf0b7 Wavelet Transform: In the wavelet transform, suppose a data vector A is transformed \\ninto a numerically different data vector A' such that both A and A' vectors are of the same \\nlength. \\n\\uf0b7 Principal Component Analysis: Suppose we have a data set to be analyzed that has \\ntuples with n attributes. The principal component analysis identifies k independent tuples \\nwith n attributes that can represent the data set. \\n\\uf0b7 Attribute Subset Selection: The large data set has many attributes, some of which are \\nirrelevant to data mining or some are redundant. The core attribute subset selection \\nreduces the data volume and dimensionality. \\n2.Numerosity Reduction \\nThe numerosity reduction reduces the original data volume and represents it in a much smaller \\nform. This technique includes two types parametric and non-parametric numerosity reduction. \\ni) Parametric: Parametric numerosity reduction incorporates storing only data parameters instead \\nof the original data. One method of parametric numerosity reduction is the regression and log-\\nlinear method. \\no Regression and Log-Linear: Linear regression models a relationship between the two \\nattributes by modeling a linear equation to the data set. Suppose we need to model a linear \\nfunction between two attributes. \\ny=wx+b \\nHere, y is the response attribute, and x is the predictor attribute.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 35, 'page_label': '36'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n36 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nLog-linear model discovers the relation between two or more discrete attributes in the database. \\nSuppose we have a set of tuples presented in n -dimensional space. Then the log -linear model is \\nused to study the probability of each tuple in a multidimensional space. \\nii) Non -Parametric: A non -parametric numerosity reduction technique does not assume any \\nmodel. The non -Parametric technique results in a more uniform reduction, irrespective of d ata \\nsize, but it may not achieve a high volume of data reduction like the parametric.  \\nNon-Parametric data reduction techniques, Histogram, Clustering, Sampling, Data Cube \\nAggregation, and Data Compression. \\n\\uf0b7 Histogram: A histogram is a graph that represents frequency distribution which describes \\nhow often a value appears in the data. Histogram uses the binning method to represent an \\nattribute's data distribution. It uses a disjoint subset which we call bin or buckets. \\n\\uf0b7 Clustering: Clustering techniques groups similar objects from the data so that the objects \\nin a cluster are similar to each other, but they are dissimilar to objects in another cluster. \\n\\uf0b7 Sampling: One of the methods used for data reduction is sampling, as it can reduce the \\nlarge data set into a much smaller data sample.  \\n\\uf0b7 Cluster sample: The tuples in data set D are clustered into M mutually disjoint subsets. \\nThe data reduction can be applied by implementing SRSWOR on these clusters. A simple \\nrandom sample of size s could be generated from these clusters where s<M. \\n\\uf0b7 Stratified sample: The large data set D is partitioned into mutually disjoint sets called \\n'strata'. A simple random sample is taken from each stratum to get stratified data. This \\nmethod is effective for skewed data. \\n3. Data Cube Aggregation \\nThis technique is used to aggregate data in a simpler form. Data Cube Aggregation is a \\nmultidimensional aggregation that uses aggregation at various levels of a data cube to represent \\nthe original data set, thus achieving data reduction.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 36, 'page_label': '37'}, page_content='INTRODUCTION TO DATA SCIENCE \\n37 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\n4. Data Compression \\nData compression employs modification, encoding, or converting the structure of data in a way \\nthat consumes less space. Data compression involves building a compact representation of \\ninformation by removing redundancy and representing data in binary form. Data that can be \\nrestored successfully from its compressed form is called Lossless compression.  \\n \\n \\ni. Lossless Compression: Encoding techniques (Run Length Encoding) allow a simple and \\nminimal data size reduction. Lossless data compression uses a lgorithms to restore the \\nprecise original data from the compressed data. \\nii. Lossy Compression: In lossy-data compression, the decompressed data may differ from \\nthe original data but are useful enough to retrieve information from them. For example, the'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 37, 'page_label': '38'}, page_content='INTRODUCTION TO DATA SCIENCE \\n38 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nJPEG image format is a lossy compression, but we can find the meaning equivalent to the \\noriginal image. Methods such as the Discrete Wavelet transform technique PCA (principal \\ncomponent analysis) are examples of this compression. \\n5. Discretization Operation \\nThe data discretization technique is used to divide the attributes of the continuous nature into data \\nwith intervals. We replace many constant values of the attributes with labels of small intervals. \\nThis means that mining results are shown in a concise and easily understandable way. \\ni. Top-down discretization:  If you first consider one or a couple of points (so -called \\nbreakpoints or split points) to divide the whole set of attributes and repeat this method up \\nto the end, then the process is known as top-down discretization, also known as splitting. \\nii. Bottom-up discretization:  If you first consider all the constant values as split -points, \\nsome are discarded through a combination of the neighborhood values in the interval. That \\nprocess is called bottom-up discretization. \\nData Discretization \\nData discretization refers to a method of converting a huge number of data values into \\nsmaller ones so that the evaluation and management of data become easy. In other words, data \\ndiscretization is a method of converting attribu tes values of continuous data into a finite set of \\nintervals with minimum data loss.  \\nThere are two forms of data discretization first is supervised discretization, and the second is \\nunsupervised discretization. \\n Supervised discretization refers to a method in which the class data is used. Unsupervised \\ndiscretization refers to a method depending upon the way which operation proceeds. It means it \\nworks on the top-down splitting strategy and bottom-up merging strategy. \\nNow, we can understand this concept with the help of an example \\nSuppose we have an attribute of Age with the given values'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 38, 'page_label': '39'}, page_content='INTRODUCTION TO DATA SCIENCE \\n39 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nAge 1,5,9,4,7,11,14,17,13,18, 19,31,33,36,42,44,46,70,74,78,77 \\nTable before Discretization \\nAttribute Age Age Age Age \\n 1,5,4,9,7 11,14,17,13,18,19 31,33,36,42,44,46 70,74,77,78 \\nAfter Discretization Child Young Mature Old \\nSome Famous techniques of data discretization \\nHistogram analysis \\nHistogram refers to a plot used to represent the underlying frequency distribution of a continuous \\ndata set. Histogram assists the data inspection for data distribution. For example, Outliers, \\nskewness representation, normal distribution representation, etc. \\nBinning \\nBinning refers to a data smoothing technique that helps to group a huge number of continuous \\nvalues into smaller values. For data discretization and the development of idea hierarchy, this \\ntechnique can also be used. \\nCluster Analysis \\nCluster analysis is a form of data discretization. A clustering algorithm is executed by dividing the \\nvalues of x numbers into clusters to isolate a computational feature of x. \\nData discretization using decision tree analysis \\ndata discretization refers to a decision tree analysis in which a top-down slicing technique is used. \\nIt is done through a supervised procedure. In a numeric attribute discretization, first, you need to \\nselect the attribute that has the least entropy, and then you need to run it with the help of a \\nrecursive process. \\nData discretization and concept hierarchy generation'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 39, 'page_label': '40'}, page_content='INTRODUCTION TO DATA SCIENCE \\n40 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nThe term hierarchy represents an organizational structure or mapping in which items are ranked \\naccording to their levels of importance. In other words, we can say that a hierarchy concept refers \\nto a sequence of mappings with a set of more general concepts to complex concepts. It means \\nmapping is done from low-level concepts to high-level concepts. \\nTop-down mapping \\nTop-down mapping generally starts with the top with some general information and ends with the \\nbottom to the specialized information. \\nBottom-up mapping \\nBottom-up mapping generally starts with the bottom with some specialized information and ends \\nwith the top to the generalized information. \\n \\n \\n \\nUNIT 3 \\nDESCRIPTIVE STATISTICS \\nDescriptive statistics: \\nDescriptive statistics describe, show, and summarize the basic features of a dataset found in a \\ngiven study, presented in a summary that describes the data sample and its measurements. It helps \\nanalysts to understand the data better. \\nDescriptive statistics represent the available data sample and does not include theories, inferences, \\nprobabilities, or conclusions. That’s a job for inferential statistics. \\nIf you want a good example of descriptive statistics, look no further than a student’s grade point \\naverage (GPA). A GPA gathers the data points created through a large selection of grades, \\nclasses, and exams, then averages them together and presents a general idea of the student’s mean'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 40, 'page_label': '41'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n41 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nacademic performance. Note that the GPA doesn’ t predict future performance or present any \\nconclusions. \\nTypes of Descriptive Statistics \\nDescriptive statistics break down into several types, characteristics, or measures. Some authors \\nsay that there are two types. Others say three or even four. In the sp irit of working with averages, \\nwe will go with three types. \\n\\uf0b7 Distribution, which deals with each value’s frequency \\n\\uf0b7 Central tendency, which covers the averages of the values \\n\\uf0b7 Variability (or dispersion), which shows how spread out the values are \\nDistribution (also called Frequency Distribution) \\nDatasets consist of a distribution of scores or values. Statisticians use graphs and tables to \\nsummarize the frequency of every possible value of a variable, rendered in percentages or \\nnumbers. For instance, if you held  a poll to determine people’s favorite Beatle, you’d set up one \\ncolumn with all possible variables (John, Paul, George, and Ringo), and another with the number \\nof votes. \\nStatisticians depict frequency distributions as either a graph or as a table. \\nMeasures of Central Tendency \\nMeasures of central tendency estimate a dataset's average or center, finding the result using three \\nmethods: mean, mode, and median. \\nMean. The mean is also known as “M” and is the most common method for finding averages. \\nYou get the mean by adding all the response values together, dividing the sum by the number of \\nresponses, or “N.” For instance, say someone is trying to figure out how many hours a day they \\nsleep in a week. So, the data set would be the hour entries (e.g., 6,8,7,10,8,4, 9), and the sum of \\nthose values is 52. There are seven responses, so N=7. You divide the value sum of 52 by N, or 7, \\nto find M, which in this instance is 7.3. \\nMode. The mode is just the most frequent response value. Datasets may have any number of \\nmodes, including “zero.” You can find the mode by arranging your dataset's order from the lowest\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 41, 'page_label': '42'}, page_content='INTRODUCTION TO DATA SCIENCE \\n42 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nto highest value and then looking for the most common response. So, in using our sleep study \\nfrom the last part: 4,6,7,8,8,9,10. As you can see, the mode is eight. \\nMedian. Finally, we have the median, defined as the value in the precise center of the dataset. \\nArrange the values in ascending order (like we did for the mode) and look for the number in the \\nset’s middle. In this case, the median is eight. \\nVariability (also called Dispersion) \\nThe measure of variability gives the statistician an idea of how spread out the responses are. The \\nspread has three aspects — range, standard deviation, and variance. \\nRange. Use range to determine how far apart the most extreme values are. Start by subtracting the \\ndataset’s lowest value from its highest value. Once again, we turn to our sleep study: \\n4,6,7,8,8,9,10. We subtract four (the lowest) from ten (the highest) and get six. There’s your \\nrange. \\nStandard Deviation. This aspect takes a  little more work. The standard deviation (s) is your \\ndataset’s average amount of variability, showing you how far each score lies from the mean. The \\nlarger your standard deviation, the greater your dataset’s variable. Follow these six steps: \\n1. List the scores and their means. \\n2. Find the deviation by subtracting the mean from each score. \\n3. Square each deviation. \\n4. Total up all the squared deviations. \\n5. Divide the sum of the squared deviations by N-1. \\n6. Find the result’s square root. \\nExample: we turn to our sleep study: 4,6,7,8,8,9,10. \\nRaw Number/Data Deviation from Mean Deviation Squared \\n4 4-7.3= -3.3 10.89'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 42, 'page_label': '43'}, page_content='INTRODUCTION TO DATA SCIENCE \\n43 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n6 6-7.3= -1.3 1.69 \\n7 7-7.3= -0.3 0.09 \\n8 8-7.3= 0.7 0.49 \\n8 8-7.3= 0.7 0.49 \\n9 9-7.3=1.7 2.89 \\n10 10-7.3= 2.7 7.29 \\nM=7.3 Sum = 0.9 Square sums= 23.83 \\n \\nWhen you divide the sum of the squared deviations by 6 (N -1): 23.83/6, you get 3.971, and the \\nsquare root of that result is 1.992. As a result, we now know that each score deviates from the \\nmean by an average of 1.992 points. \\nVariance: Variance reflects the dataset’s degree spread. The greater the degree of data spread, the \\nlarger the variance relative to the mean. You can get the variance by just squaring the standard \\ndeviation. Using our above example, we square 1.992 and arrive at 3.971.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 43, 'page_label': '44'}, page_content='INTRODUCTION TO DATA SCIENCE \\n44 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nSkewness: \\nSkewness is the measure of the asymmetry of an ideally symmetric probability distribution and is \\ngiven by the third standardized moment. If that sounds way too complex, don’t worry! Let me \\nbreak it down for you. \\n \\nIn simple words, skewness is the measure of how muc h the probability distribution of a random \\nvariable deviates from the normal distribution.  \\nthe normal distribution is the probability distribution without any skewness . You can look at the \\nimage below which shows symmetrical distribution that’s basically a normal distribution and you \\ncan see that it is symmetrical on both sides of the dashed line. Apart from this, there are two types \\nof skewness: \\n\\uf0b7 Positive Skewness \\n\\uf0b7 Negative Skewness'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 44, 'page_label': '45'}, page_content='INTRODUCTION TO DATA SCIENCE \\n45 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nThe probability distribution with its tail on the right side is a positively skewed distribution and \\nthe one with its tail on the left side is a negatively skewed distribution. \\n\\uf0b7 Normal Distribution: Normal Distribution is a probability distribution that is symmetric about \\nthe mean. It is also known as Gaussian Distribution. The distribution appears as a Bell -shaped \\ncurve which means the mean is the most frequent data in the given data set. \\n \\n \\nIn Normal Distribution : \\nMean = Median = Mode \\n\\uf0b7 Standard Normal Distribution: When in the Normal Distribution mean = 0 and the Standard \\nDeviation = 1 then Normal Distribution is called as Standard Normal Distribution. \\n\\uf0b7 Normal Distributions are symmetrical in nature it doesn’t imply that every symmetrical \\ndistribution is a Normal Distribution. \\n\\uf0b7 Normal Distribution is the probability distribution without any skewness.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 45, 'page_label': '46'}, page_content='INTRODUCTION TO DATA SCIENCE \\n46 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nTypes of Skewness \\n \\n\\uf0b7 Positive Skewness \\n\\uf0b7 Negative Skewness \\nUnlike the Normal Distribution (mean = median = mode), in positive as well as negative \\nskewness mean, median, and mode all are different. \\nPositive Skewness \\nIn positive skewness, the extreme data values are larger, which in turn increase the mean value of \\nthe data set, or in the simple term in positive skew distribution is the distribution having the tail on \\nthe right side. \\nIn Positive Skewness: \\nMean > Median > Mode \\nNegative Skewness \\nIn negative skewness, the extreme data values are smaller, which decreases the mean value of the \\ndataset or the negative skew distribution is the distribution having the tail on the left side. \\nIn Negative Skewness: \\nMean <  Median< Mode'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 46, 'page_label': '47'}, page_content='INTRODUCTION TO DATA SCIENCE \\n47 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\n \\nCalculate the skewness coefficient of the sample \\nPearson’s first coefficient of skewness \\nSubtract a mode from a mean, then divides the difference by standard deviation. \\n \\nAs Pearson’s correlation coefficient differs from -1 (perfect negative linear relationship) to +1 \\n(perfect positive linear relationship), including a value of 0 indicating no linear relationship, \\nWhen we divide the covariance values by the standard deviation, it truly scales the value down to \\na limited range of -1 to +1. That accurately the range of the correlation values. \\nPearson’s second coefficient of skewness \\nMultiply the difference by 3, and divide the product by standard deviation.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 47, 'page_label': '48'}, page_content='INTRODUCTION TO DATA SCIENCE \\n48 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nIf the skewness is between -0.5 & 0.5, the data are nearly symmetrical. \\nIf the skewness is between -1 & -0.5 (negative skewed) or between 0.5 & 1(positive skewed), the \\ndata are slightly skewed. \\nIf the skewness is lower than -1 (negative skewed) or greater than 1 (positive ske wed), the data \\nare extremely skewed. \\n \\nKurtosis: \\nKurtosis measures the \"heaviness of the tails\" of a distribution (in compared to a normal \\ndistribution). Kurtosis is positive if the tails are \"heavier\" then for a normal distribution,  and \\nnegative if the tails are \"lighter\" than for a normal distribution. The normal distribution has \\nkurtosis of zero. \\nKurtosis characterizes the shape of a distribution - that is, its value does not depend on an \\narbitrary change of the scale and location of the distribution. For example, kurtosis of a sample (or \\npopulation) of temperature values in Fahrenheit will not change if you transform the values to \\nCelsius (the mean and the variance will, however, change). \\nThe kurtosis of a distribution or sample is  equal to the 4th central  moment divided by the 4th \\npower of the standard deviation, minus 3. \\nTo calculate the kurtosis of a sample: \\ni) subtract the mean from each value to get a set of deviations from the mean; \\nii) divide each deviation by the standard deviation of all the deviations; \\niii) average the 4th power of the deviations and subtract 3 from the result.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 48, 'page_label': '49'}, page_content='INTRODUCTION TO DATA SCIENCE \\n49 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nExcess Kurtosis \\nThe excess kurtosis is used in statistics and probability theory to compare the kurtosis coefficient \\nwith that normal distribution. Excess kurtosis can be positive (Leptokurtic dis tribution), negative \\n(Platykurtic distribution), or near to zero (Mesokurtic distribution). Since normal distributions \\nhave a kurtosis of 3, excess kurtosis is calculating by subtracting kurtosis by 3. \\n               Excess kurtosis  =  Kurt – 3 \\nTypes of excess kurtosis \\n1. Leptokurtic or heavy-tailed distribution (kurtosis more than normal distribution). \\n2. Mesokurtic (kurtosis same as the normal distribution). \\n3. Platykurtic or short-tailed distribution (kurtosis less than normal distribution). \\nLeptokurtic (kurtosis > 3) \\nLeptokurtic is having very long and skinny tails, which means there are more chances of outliers. \\nPositive values of kurtosis indicate that distribution is peaked and possesses thick tails. An \\nextreme positive kurtosis indicates a distribution where more of the numbers are located in the \\ntails of the distribution instead of around the mean.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 49, 'page_label': '50'}, page_content='INTRODUCTION TO DATA SCIENCE \\n50 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nplatykurtic (kurtosis < 3) \\nPlatykurtic having a lower tail and stretched around center tails means most of the data points are \\npresent in high  proximity with mean. A platykurtic distribution is flatter (less peaked) when \\ncompared with the normal distribution. \\nMesokurtic (kurtosis = 3) \\nMesokurtic is the same as the normal distribution, which means kurtosis is near to 0. In \\nMesokurtic, distributions are moderate in breadth, and curves are a medium peaked height.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 50, 'page_label': '51'}, page_content='INTRODUCTION TO DATA SCIENCE \\n51 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nBox plot: \\nA box plot also known as Five Number Summary, summarizes data using the median, upper \\nquartile, lower quartile, and the minimum and maximum values. It allows you to see impor tant \\ncharacteristics of the data at a glance(visually). This also help us to visualize outliers in the data \\nset. \\nBox plot or Five Number Summary has below five information. \\n1.Median \\n2. Lower Quartile(25th Percentile) \\n3.Upper Quartile(75th Percentile) \\n4. Minimum Value \\n5.Maximum Value'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 51, 'page_label': '52'}, page_content='INTRODUCTION TO DATA SCIENCE \\n52 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nWorking Example of Box \\nLet’s understand Box plot with this an example. \\nStep 1 — take the set of numbers given \\n14, 19, 100, 27, 54, 52, 93, 50, 61, 87,68, 85, 75, 82, 95 \\nArrange the data in increasing(ascending) order \\n14, 19, 27, 50, 52, 54, 61, 68, 75, 82, 85, 87, 93, 95, 100 \\nStep 2 — Find the median of this data set. Median is mid value in this ordered data set.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 52, 'page_label': '53'}, page_content='INTRODUCTION TO DATA SCIENCE \\n53 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n14, 19, 27, 50, 52, 54, 61, 68, 75, 82, 85, 87, 93, 95, 100 \\nHere it is 68. \\nStep 3 — Lets find the Lower Quartile. \\nLower Quartile is the median from the left of the, medium found in the Step 2(ie. 68) \\n(14, 19, 27, 50, 52, 54, 61), 68, 75, 82, 85, 87, 93, 95, 100 \\nLower Quartile is 50 \\nStep 4 — Lets find the Upper Quartile. \\nUpper Quartile is the median from the Right of the medium found in the Step 2(ie. 68) \\n14, 19, 27, 50, 52, 54, 61, 68,( 75, 82, 85, 87, 93, 95, 100) \\nUpper Quartile is 87 \\nStep 5 — Lets find the Minimum Value \\nIt is value lies in the extreme left from this data set or first value in the data set after ordering. \\n14, 19, 27, 50, 52, 54, 61, 68, 75, 82, 85, 87, 93, 95, 100'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 53, 'page_label': '54'}, page_content='INTRODUCTION TO DATA SCIENCE \\n54 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nMinimum Value is 14 \\nStep 6 — Lets find the Maximum Value \\nIt is value lies in the extreme Right from this data set or last value in the data set after ordering. \\n14, 19, 27, 50, 52, 54, 61, 68, 75, 82, 85, 87, 93, 95, 100 \\nMaximum Value is 100 \\n \\nRange : \\nRange is basically spread of our data set.Range can be found as difference between Maximum \\nValue and Minimum Values.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 54, 'page_label': '55'}, page_content='INTRODUCTION TO DATA SCIENCE \\n55 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nInterquartile Range(IQR): \\nInterquartile Range(IQR) is difference between Upper quartile and Lower quartile. \\nAs per picture above,our Lower quartile is 50 and Upper quartile is 87 \\n \\nBox plot with Even numbers of data set : \\nStep 1 : \\nWe have 14 records below. \\n14, 19, 100, 27, 54, 52, 93, 50, 61, 87,68, 85, 75, 82 \\nArrange the data in increasing(ascending) order \\n14, 19, 27, 50, 52, 54, 61, 68, 75, 82, 85, 87, 93, 100 \\nStep 2:Since we the even number take the middle two values add them and divide them by 2. \\nHere Values at position 7 & 8 are middle values.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 55, 'page_label': '56'}, page_content='INTRODUCTION TO DATA SCIENCE \\n56 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nSo our new median value is 64.5 \\n \\nContinue Step 3 to Step 6 to get the values mention in  Working Example of Box \\nPlot section.Final Result as below \\n \\n \\nPivot table: \\nA pivot table is a  summary tool that wraps up or summarizes information sourced from bigger \\ntables. These bigger tables could be a database, an Excel spreadsheet, or any data that is or could \\nbe converted in a table -like form. The data summarized in a pivot table might include sums, \\naverages, or other statistics which the pivot table groups together in a meaningful way.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 56, 'page_label': '57'}, page_content='INTRODUCTION TO DATA SCIENCE \\n57 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nWide vs. Long Data \\nPivoting is the act of converting “long data” into “wide data”. Wide and long data formats serve \\ndifferent purposes. It’s often helpful to think of how data might be collected in the first place. \\nRepresenting potentially multi-dimensional data in a single 2-dimensional table may require some \\ncompromises. Either some data will be repeated (long data format) or your data set may require \\nblank cells (wide data). \\nExamples of pivot table: \\n\\uf0b7 Pivoting in Python with Pandas \\nStarting with the raw dataset loaded into df_long_data \\n \\n \\nNote the key arguments here.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 57, 'page_label': '58'}, page_content='INTRODUCTION TO DATA SCIENCE \\n58 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n1. Index determines what will be unique in the leftmost column of the result. \\n2. Columns creates a column for each unique value in the “Vehicle type” and “Year” columns \\nof the input table. \\n3. Values defines what to put in the cells of the output table. \\n4. aggfunc defines how to combine the values (commonly sum, but other aggregation functions \\nare also used: min, max, mean, 95th percentile, mode). You can also define your own \\naggregation functions and pass in the function. \\n\\uf0b7 Pivoting in Google Sheets \\nPivoting in Google sheets is doing the exact same thing as our python code above but in a \\ngraphical way. Instead of specifying arguments for the  pivot_table function, we select from \\ndropdowns. But the important thing t o remember is pivoting doesn’t “belong” to any particular \\nsoftware, it’s a generalized approach for dealing with data. \\n\\uf0b7 Grouping in PostgreSQL \\nMany relational databases don’t have a built in pivot function. We can write a query that \\napproximates the desired  results but it does require some manual intervention to define the \\npossible groups. \\nAggregation Functions \\nSum is often used to combine data in pivot tables but pivot tables are much more flexible than just \\nsimple sums. Different tools will provide their own selection of  aggregation functions.  For \\nexample, Pandas provides: min, max, first, last, unique, std (standard deviation),'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 58, 'page_label': '59'}, page_content='INTRODUCTION TO DATA SCIENCE \\n59 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nvar (variance), count, unique, quantile  among others. We can also define our own \\naggregation functions and pass multiple differen t aggregation functions to the same column or \\ndifferent columns in the same pivot table. \\nMissing Data \\nAs with any aggregation missing data must be dealt with. In the example data used here there are \\nnumerous months with  0 sales. The most probably explanati on is that data wasn’t collected for \\nthose month, not that there were actually zero sales. There’s also the scenario where data is \\nmissing entirely and the “cell” is blank or contains a  null value. Whatever the program you’ll \\nneed to accept (and be aware of) the default behaviour for these cases or specify what to do. \\n \\nHeat map: \\nHeatmaps visualize the data in a 2 -dimensional format in the form of colored maps. The color \\nmaps use hue, saturation, or luminance to achieve color variation to display various details. This \\ncolor variation gives visual cues to the readers about the magnitude of numeric values. \\nHeatmaps can describe the density or intensity of variables, visualize patterns, variance, and even \\nanomalies. Heatmaps show relationships between variables. These variables are plotted on both \\naxes. \\nUses of HeatMap \\nBusiness Analytics: A heat map is used as a visual business analytics tool. A heat map gives \\nquick visual cues about the current results, performance, and scope for improvements. Heatmaps \\ncan analyze the existing data and find areas of intensity that might reflect where most cust omers \\nreside, areas of risk of market saturation, or cold sites and sites that need a boost. Heat maps can'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 59, 'page_label': '60'}, page_content='INTRODUCTION TO DATA SCIENCE \\n60 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nbe continued to be updated to reflect the growth and efforts. These maps can be integrated into a \\nbusiness’s workflow and become a part of ongoing analytics.  \\n\\uf0b7 Website: Heatmaps are used in websites to visualize data of visitors’ behavior. This \\nvisualization helps business owners and marketers to identify the best & worst -performing \\nsections of a webpage. These insights help them with optimization. \\n\\uf0b7 Exploratory Data Analysis:  EDA is a task performed by data scientists to get familiar \\nwith the data. All the initial studies are done to understand the data are known as EDA. \\nExploratory Data Analysis (EDA) is the process of analyzing datasets before the modeling \\ntask. It is a tedious task to look at a spreadsheet filled with numbers and determine \\nessential characteristics in a dataset.  \\n\\uf0b7 Molecular Biology: Heat maps are used to study disparity and similarity patterns in DNA, \\nRNA, etc. \\n\\uf0b7 Geovisualization: Geospatial heatmap charts are useful for displaying how geographical \\nareas of a map are compared to one another based on specific criteria. Heatmaps help in \\ncluster analysis or hotspot analysis to detect clusters of high concentrations of activity; For \\nexample, Airbnb rental price analysis. \\n\\uf0b7 Marketing and Sales: The heatmap’s capability to detect warm and cold spots is used to \\nimprove marketing response rates by targeted marketing. Heatmaps allow the detection of \\nareas that respond to campaigns, under-served markets, customer residence, and high sale \\ntrends, which helps optimize product lineups, capitalize on sales, create targeted customer \\nsegments, and assess regional demographics. \\nTypes of HeatMaps \\nTypically, there are two types of Heatmaps: \\n\\uf0b7 Grid Heatmap: The magnitudes of values shown through colors are laid out into a matrix of \\nrows and columns, mostly by a density -based function. Below are the types of Grid \\nHeatmaps.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 60, 'page_label': '61'}, page_content='INTRODUCTION TO DATA SCIENCE \\n61 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0fc Clustered Heatmap: The goal of Clustered Heatmap is to build associations between both \\nthe data points and their features. This type of heatmap implements clustering as part of \\nthe process of grouping similar features. Clustered Heatmaps are widely used in biological \\nsciences for studying gene similarities across individuals. \\nThe order of the rows in Clustered Heatmap is determined by performing hierarchical cluster \\nanalysis of the rows. Clustering positions similar rows together on the map. Similarly, the order of \\nthe columns is determined. \\n\\uf0fc Correlogram: A correlogram replaces each of the variables on the two axes with numeric \\nvariables in the dataset. Each square depicts the relationship between the two intersecting \\nvariables, which helps to build descriptive or predictive statistical models. \\n\\uf0b7 Spatial Heatmap: Each square in a Heatmap is assigned a color representation according to \\nthe nearby cells’ value. The location of color is according to the magnitude of the value in that \\nparticular space. These Heatmaps are data -driven “paint by numbers” canvas o verlaid on top \\nof an image. The cells with higher values than other cells are given a hot color, while cells \\nwith lower values are assigned a cold color. \\n \\nCorrelation statistics: \\nCorrelation \\n\\uf0d8 Correlation measures the relationship between two variables. \\nWe mentioned that a function has a purpose to predict a value, by converting input (x) to output \\n(f(x)). We can say also say that a function uses the relationship between two variables for \\nprediction. \\nCorrelation Coefficient \\nThe correlation coefficient measures the relationship between two variables. \\nThe correlation coefficient can never be less than -1 or higher than 1. \\n\\uf0d8 1 = there is a perfect linear relationship between the variables (like Average_Pulse against \\nCalorie_Burnage)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 61, 'page_label': '62'}, page_content='INTRODUCTION TO DATA SCIENCE \\n62 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0d8 0 = there is no linear relationship between the variables \\n\\uf0d8 -1 = there is a perfect negative linear relationship between the variables (e.g. Less hours \\nworked, leads to higher calorie burnage during a training session) \\nExample of a Perfect Linear Relationship (Correlation Coefficient = 1) \\n\\uf0b7 We will use scatterplot to visualize the relationship between Average_Pulse and \\nCalorie_Burnage (we have used the small data set of the sports watch with 10 \\nobservations). \\n\\uf0b7 This time we want scatter plots, so we change kind to \"scatter\": \\n#Three lines to make our compiler able to draw: \\nimport sys \\nimport matplotlib \\nmatplotlib.use(\\'Agg\\') \\n \\nimport pandas as pd \\nimport matplotlib.pyplot as plt \\n \\nhealth_data = pd.read_csv(\"data.csv\", header=0, sep=\",\") \\n \\nhealth_data.plot(x =\\'Average_Pulse\\', y=\\'Calorie_Burnage\\', kind=\\'scatter\\'), \\n \\nplt.show() \\n \\n#Two  lines to make our compiler able to draw: \\nplt.savefig(sys.stdout.buffer) \\nsys.stdout.flush() \\n \\noutput: \\n \\nExample of a Perfect Negative Linear Relationship (Correlation Coefficient = -1)'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 62, 'page_label': '63'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n63 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n#Three lines to make our compiler able to draw: \\nimport sys \\nimport matplotlib \\nmatplotlib.use('Agg') \\nimport pandas as pd \\nimport matplotlib.pyplot as plt \\nnegative_corr = {'Hours_Work_Before_Training': [10,9,8,7,6,5,4,3,2,1], \\n'Calorie_Burnage': [220,240,260,280,300,320,340,360,380,400]} \\nnegative_corr = pd.DataFrame(data=negative_corr) \\nnegative_corr.plot(x ='Hours_Work_Before_Training', y='Calorie_Burnage', kind='scatter') \\nplt.show() \\n#Two  lines to make our compiler able to draw: \\nplt.savefig(sys.stdout.buffer) \\nsys.stdout.flush() \\noutput:\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 63, 'page_label': '64'}, page_content='INTRODUCTION TO DATA SCIENCE \\n64 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nWe have plotted fictional data here. The x-axis represents the amount of hours worked at our job \\nbefore a training session. The y-axis is Calorie_Burnage. \\nIf we work longer hours, we tend to have lower calorie burnage because we are exhausted before \\nthe training session. \\nThe correlation coefficient here is -1. \\nExample of No Linear Relationship (Correlation coefficient = 0) \\n#Three lines to make our compiler able to draw: \\nimport sys \\nimport matplotlib \\nmatplotlib.use(\\'Agg\\') \\nimport pandas as pd \\nimport matplotlib.pyplot as plt \\nfull_health_data = pd.read_csv(\"data.csv\", header=0, sep=\",\")'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 64, 'page_label': '65'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n65 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nfull_health_data.plot(x ='Duration', y='Max_Pulse', kind='scatter') \\nplt.show() \\n#Two lines to make our compiler able to draw: \\nplt.savefig(sys.stdout.buffer) \\nsys.stdout.flush() \\noutput: \\n \\nANOVA: \\nANOVA stands for analysis of variance and, as the name suggests, it helps us understand and \\ncompare variances among groups. Before going in detail about ANOVA, let’s remember a few \\nterms in statistics: \\n\\uf0b7 Mean: The average of all values.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 65, 'page_label': '66'}, page_content='INTRODUCTION TO DATA SCIENCE \\n66 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Variance: A measure of the variation among values. It is calculated by adding up squared \\ndifferences of each value and the mean and then dividing the sum by the number of samples. \\n \\n\\uf0b7 Standard deviation: The square root of variance. \\nIn order to understand the mo tivation behind ANOVA, or some other statistical tests, we should \\nlearn two simple terms: population and sample. \\nPopulation is all elements in a group. For example, \\n\\uf0b7 College students in US is a population that includes all of the college students in US. \\n\\uf0b7 25-year-old people in Europe is a population that includes all of the people that fits the \\ndescription. \\nIt is not always feasible or possible to do analysis on population because we cannot collect all the \\ndata of a population. Therefore, we use samples. \\nSample is a subset of a population. For example,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 66, 'page_label': '67'}, page_content='INTRODUCTION TO DATA SCIENCE \\n67 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 1000 college students in US is a subset of “college students in US” population. \\nWhen we compare two samples (or groups), we can use  t-test to see if there is any difference in \\nmeans of groups. When we have mor e than two groups, t -test is not the optimal choice because \\nwe need to apply t -test to pairs separately. Consider we have groups A, B and C. To be able to \\ncompare the means, we need to apply a t -test to A -B, A-C and B -C. As the number of groups \\nincrease, this becomes harder to manage. \\nIn the case of comparing three or more groups, ANOVA is preferred. There are two elements of \\nANOVA: \\n\\uf0b7 Variation within each group \\n\\uf0b7 Variation between groups \\nANOVA test result is based on F ratio which is the ratio of the variation  between groups to the \\nvariation within groups. \\n \\nF ratio shows how much of the total variation comes from the variation between groups and how \\nmuch comes from the variation within groups.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 67, 'page_label': '68'}, page_content='INTRODUCTION TO DATA SCIENCE \\n68 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nUNIT – 4 \\nMODEL DEVELOPMENT \\nRegression Analysis \\nRegression analysis is a predictive modelling technique that assesses the relationship \\nbetween dependent (i.e., the goal/target variable) and independent factors. Forecasting, time series \\nmodelling, determining the relationship between variables, and predicting conti nuous values can \\nall be done using regression analysis. Just to give you an Analogy, Regression is the best way to \\nstudy the relationship between household areas and a driver’s household electricity cost.  \\nNow, These Regression falls under 2 Categories Namely,  \\n\\uf0b7 Simple Linear Regression:  The association between two variables is established using a \\nstraight line in Simple Linear Regression. It tries to create a line that is as near to the data as \\npossible by determining the slope and intercept, which define t he line and reduce regression \\nerrors. There is a single x and y variable   \\nEquation: Y = mX+c  \\n\\uf0b7 Multiple Linear Regression:  Multiple linear regressions are based on the presumption that \\nboth the dependent and independent variables, or Predictor and Target v ariables, have a linear \\nrelationship. There are two types of multilinear regressions: linear and nonlinear. It has one or \\nmore x variables and one or more y variables, or one dependent variable and two or more \\nindependent variables  \\n\\uf0b7 Equation: Y = m1X1+m2X2+m3X3+..c  \\n\\uf0b7 Where,  \\n\\uf0b7 Y\\u202f=\\u202fDependent\\u202fVariable  \\nm\\u202f=\\u202fSlope\\u202f  \\nX\\u202f=\\u202fIndependent\\u202fVariable  \\nc\\u202f=\\u202fIntercept  \\n\\uf0b7 Now, let us understand both Simple and Multiple Linear Regression implementation with \\nthe below sample datasets!!  \\nSimple Linear Regression  \\nGiven the experience let us predict the salary of the employees using a simple Regression model \\nimport pandas as pd'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 68, 'page_label': '69'}, page_content='INTRODUCTION TO DATA SCIENCE \\n69 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nimport numpy as np \\nimport matplotlib.pyplot as plt \\nfrom sklearn.linear_model import LinearRegression \\nRead the data from the csv file  \\ndata = pd.read_csv(\\'/content/Salary_Data.csv\\') #reading data \\ndata.head() \\n \\n\\uf0d8 Plot Years of Experience vs Salary based on Experience  \\nplt.figure() \\nplt.scatter(data[\\'YearsExperience\\'],data[\\'Salary\\'],c=\\'black\\') \\nplt.xlabel(\"YearsExperience\") \\nplt.ylabel(\"Salary\") \\nplt.show()'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 69, 'page_label': '70'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n70 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\n\\uf0d8 Reshape and fit the data into simple linear regression  \\nX = data['YearsExperience'].values.reshape(-1,1) \\ny = data['Salary'].values.reshape(-1,1) \\nreg = LinearRegression() \\nreg.fit(X, y) \\nPrint R-squared value \\n \\nMultiple Linear Regression  \\nLet us the dataset set of advertising sales based on Tv and Newspaper\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 70, 'page_label': '71'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n71 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nImport required libraries  \\n%matplotlib inline \\nimport numpy as np \\nimport pandas as pd \\nimport matplotlib.pyplot as plt \\nplt.rcParams['figure.figsize'] = (15.0, 8.0) \\nfrom mpl_toolkits.mplot3d import Axes3D \\nfrom sklearn.model_selection import train_test_split  \\nfrom sklearn.linear_model import LinearRegression \\nfrom sklearn.metrics import mean_squared_error \\nfrom sklearn.metrics import r2_score \\n \\nRead dataset from csv file  \\ndata = pd.read_csv('/content/Advertising Agency.csv') #reading data \\ndata.head() \\n \\ntv = data['TV'].values #storing dataframe values as variables \\nnewspaper = data['Newspaper'].values \\nsales = data['Sales'].values\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 71, 'page_label': '72'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n72 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nPlotting Actual vs Predicted values \\nfig = plt.figure(2) \\nax = Axes3D(fig) \\nax.scatter(X_train[:,0], X_train[:,1], y_train, color='r',label='Actual Values') \\nax.scatter(X_test[:,0],X_test[:,1], y_pred, color='b',label='Predicted Values') \\nax.set_xlabel('TV') \\nax.set_ylabel('Newspaper') \\nax.set_zlabel('Sales through TV and Newspaper') \\nax.legend() \\nplt.show()\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 72, 'page_label': '73'}, page_content='INTRODUCTION TO DATA SCIENCE \\n73 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nMultiple regression in machine learning: \\nMultiple Linear Regression is one of the important regression algorithms which models the linear \\nrelationship between a single dependent continuous variable and more than one independent \\nvariable. \\nExample: \\nPrediction of CO2 emission based on engine size and number of cylinders in a car. \\nMLR equation: \\nIn Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor \\nvariables x1, x2, x3, ...,xn. Since it is an enhancement of Simple Linear Regression, so the same is \\napplied for the multiple linear regression equation, the equation becomes: \\nY= b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+ b<sub>2</sub>x<sub>2</sub>+ b<sub>3\\n</sub>x<sub>3</sub>+...... bnxn    \\nWhere, \\nY= Output/Response variable \\nb0, b1, b2, b3 , bn....= Coefficients of the model. \\nx1, x2, x3, x4,...= Various Independent/feature variable'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 73, 'page_label': '74'}, page_content='INTRODUCTION TO DATA SCIENCE \\n74 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nModel evaluation using visualization \\nResidual Plot:  \\nResiduals \\nA residual is a measure of how far away a point is vertically from the regression line. Simply, it is \\nthe error between a predicted value and the observed actual value. \\n \\n \\nThe above fig is an example of how to visualize residuals against the line of best fit. The vertical \\nlines are the residuals. \\nResidual Plots \\nA typical residual plot has the residual values on the Y-axis and the independent variable on the x-\\naxis. The below fig \\nis a good example of how a typical residual plot looks like.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 74, 'page_label': '75'}, page_content='INTRODUCTION TO DATA SCIENCE \\n75 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nResidual Plot Analysis \\nThe most important assumption of a linear regression model is that the errors are independent \\nand normally distributed. \\nLet’s examine what this assumption means. \\nEvery regression model inherently has some degree of error since you can never predict \\nsomething 100% accurately. More importantly, randomness and unpredictability are always a part \\nof the regression model. Hence, a regression model can be explained as:'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 75, 'page_label': '76'}, page_content='INTRODUCTION TO DATA SCIENCE \\n76 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nThe deterministic part of the model is what we try to capture using the regression model. Ideally, \\nour linear equation model should accurately capture the predictive information. Essentially, what \\nthis means is that if we capture all of the predictive information, all that is left behind (residuals) \\nshould be completely random & unpredictable i.e stochastic. Hence, we want our residuals to \\nfollow a normal distribution.  \\nCharacteristics of Good Residual Plots \\nA few characteristics of a good residual plot are as follows: \\n1. It has a high density of points close to the origin and a low density of points away from the \\norigin \\n2. It is symmetric about the origin \\nTo explain why below fig is a good residual plot based on the characteristics above, we project all \\nthe residuals onto the y-axis. As seen in Figure 3b, we end up with a normally distributed curve; \\nsatisfying the assumption of the normality of the residuals. \\n                     \\nGood residual plots      project on to the Y axis'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 76, 'page_label': '77'}, page_content='INTRODUCTION TO DATA SCIENCE \\n77 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nFinally, one other reason this is a good residual plot is, that independent of the value of an \\nindependent variable (x -axis), the residual errors are approximately distributed in the same \\nmanner. In other words, we do not see any patterns in the value of the residuals as we move along \\nthe x-axis. \\nHence, this satisfies our earlier assumption that regression model residuals are independent and \\nnormally distributed. \\nUsing the characteristics  the bad residual plot: this plot has high density far away from the origin \\nand low density close to the origin. Also, when we project the residuals on the y-axis, we can see \\nthe distribution curve is not normal. \\n              \\nExample of Bad Residual plot    Project onto the y-axis \\nDistribution plots \\n \\nPolynomial Regression \\nIn polynomial regression, the relationship between the independent variable x and the dependent \\nvariable y is described as an nth degree polynomial in x. Polynomial regression, abbreviated E(y \\n|x), describes the fitting of a nonlinear relationship between the value of x and the conditional \\nmean of y. It usually corresponded to the least-squares method.  \\n \\nTypes of Polynomial Regression'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 77, 'page_label': '78'}, page_content='INTRODUCTION TO DATA SCIENCE \\n78 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nA quadratic equation is a general term for a second-degree polynomial equation. This degree, on \\nthe other hand, can go up to nth values. Polynomial regression can so be categorized as follows: \\n1. Linear – if degree as 1 \\n2. Quadratic – if degree as 2 \\n3. Cubic – if degree as 3 and goes on, on the basis of degree. \\n \\n \\nAssumption of Polynomial Regression \\nWe cannot process all of the datasets and use polynomial regression machine learning to make a \\nbetter judgment. We can still do it, but there should be specific constraints for the dataset in order \\nto get the best polynomial regression results. \\nA dependent variable’s behaviour can be described by a linear, or curved, an additive link \\nbetween the dependent variable and a set of k independent factors. \\nThe independent variables have no relationship with one another. \\nWe’re utilizing datasets with independent errors that are normally distributed with a mean of zero \\nand a constant variance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 78, 'page_label': '79'}, page_content='INTRODUCTION TO DATA SCIENCE \\n79 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nHere we are dealing with mathematics, rather than going deep, just understand the basic structure, \\nwe all know the equation of a linear equation will be a straight line, from that if we have many \\nfeatures then we opt for multiple regression just increasing features part alone, then how about \\npolynomial, it’s not about increasing but changing the structure to a quadratic equation, you can \\nvisually understand from the diagram, \\n \\nLinear Regression Vs Polynomial Regression \\nRather than focusing on the distinctions between linear and polynomial regression, we may \\ncomprehend the importance of polynomial regression by starting with linear regression. We build \\nour model and realize that it performs abysmally. We examine the difference between the actual \\nvalue and the best fit line we predicted, and it appears that the true value has a curve on the graph, \\nbut our line is nowhere near cutting the mean of the points. This is where polynomial regression \\ncomes into play; it predicts the best-fit line that matches the pattern of the data (curve). \\nOne important distinction between Linear and Polynomial Regression is that Polynomial \\nRegression does not require a linear relationship between the independent and dependent \\nvariables in the data set. When the Linear Regression Model fails to capture the points in the data'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 79, 'page_label': '80'}, page_content='INTRODUCTION TO DATA SCIENCE \\n80 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nand the Linear Regression fails to adequately represent the optimum conclusion, Polynomial \\nRegression is used. \\nNon-linear data in Polynomial Regression \\nWe need to enhance the model’s complexity to overcome under -fitting. In this sense, we need to \\nmake linear analyzes in a non-linear way, statistically by using Polynomial, \\n \\nBecause the weights associated with the features are still linear, this is still called a  linear model. \\nx2 (x square) is only a function. However, the curve we’re trying to fit is quadratic in nature. \\nData science pipeline \\nA Data Science Pipeline is a collection of processes that transform raw data into actionable \\nbusiness answers. Data Science Pipelines automate the flow of data from source to destination, \\nproviding you with insights to help you make business decisions. \\nThe Data Science Pipeline refers to the process and tools used to collect raw data from various \\nsources, analyze it, and pres ent the results in a Comprehensible Format. Companies use the \\nprocess to answer specific business questions and generate actionable insights from real -world \\ndata. To find this information, all available Datasets, both External and Internal, are analyzed. \\nFor example, your Sales Team would like to set realistic goals for the coming quarter. They can \\ncollect data from customer surveys or feedback, historical purchase orders, industry trends, and \\nother sources using the data science pipeline. Robust data analy sis tools are then used to \\nthoroughly analyze the data and identify key trends and patterns.  \\nKey Features of Data Science Pipelines \\nHere is a list of key features of the Data Science Pipeline: \\n\\uf0b7 Continuous and Scalable Data Processing \\n\\uf0b7 Cloud-based Elasticity and Agility. \\n\\uf0b7 Data Processing Resources that are Self-Contained and Isolated. \\n\\uf0b7 Access to a Large Amount of Data and the ability to self-serve. \\n\\uf0b7 Disaster Recovery and High Availability'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 80, 'page_label': '81'}, page_content='INTRODUCTION TO DATA SCIENCE \\n81 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n\\uf0b7 Allow users to Delve into Insights at a Finer Level. \\n\\uf0b7 Removes Data silos and Bottlenecks that cause Delays and Waste of Resources. \\nWorking of Data Science Pipeline: \\nIt is critical to have specific questions you want data to answer before moving raw data through \\nthe pipeline. This allows users to focus on the right data in order to uncover the right insights. \\nThe Data Science Pipeline is divided into several stages, which are as follows: \\n\\uf0b7 Obtaining Information \\n\\uf0b7 Data Cleansing \\n\\uf0b7 Data Exploration and Modeling \\n\\uf0b7 Data Interpretation \\n\\uf0b7 Data Revision \\n1) Obtaining Information \\nThis is the location where data from internal, external, and third -party sources is collected and \\nconverted into a usable format (XML, JSON, .csv, etc.). \\n2) Data Cleansing \\nThis is the most time -consuming step. Anomalies in data, such as duplicate parameters, missing \\nvalues, or irrelevant information, must be cleaned before creating a data visualization. \\n3) Data Exploration and Modeling \\nAfter thoroughly cleaning the data, it can be used to find patterns and values using data \\nvisualization tools and charts. This is where machine learning tools can help. \\n4) Data Interpretation \\nThe goal of this step is to identify insights and then correlate them to your data findings. You can \\nthen use charts, dashboards, or reports to present your findings to business leaders or colleagues. \\n5) Data Revision \\nAs business requirements change or more data becomes available, it’s critical to re visit your \\nmodel and make any necessary changes.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 81, 'page_label': '82'}, page_content='INTRODUCTION TO DATA SCIENCE \\n82 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nBenefits: \\nFollowing are the benefits of Data Science Pipelines \\n1. The pattern that can be replicated \\nIndividual pipes are patterns in a larger architecture that may be recycled and reused for \\nnew data flows when data processing is viewed as a network of pipelines. \\n2. Integration of new data sources takes less time. \\nHaving a common concept and techniques for how data should pass through analytics \\nsystems makes it simpler to plan for integrating new data sources and minimizes the time \\nand expense of integrating them. \\n3. Data quality assurance \\nUnderstanding data streams as pipelines that need to be regulated and useful to end-users \\nincreases data quality and minimizes the chances of pipeline breakdowns going \\nundiscovered. \\n4. Assurance of the pipeline’s security \\nWith repetitive patterns and consistent knowledge of tools and architectures, security is \\nbaked in from the start. Good security procedures can easily apply to new dataflows or \\ndata sources.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 82, 'page_label': '83'}, page_content='INTRODUCTION TO DATA SCIENCE \\n83 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n5. Build in stages \\nWhen you think of your dataflows as pipelines, you can scale them up gradually. You can \\nget started early and achieve benefits immediately by starting with a modest controllable \\nsegment from a data source to a user. \\n6. Agility and flexibility \\nPipelines give a structure for responding dynamically to modifications in the sources or \\nthe needs of your data users. \\nExtensible, modular, and reusable Data Pipelines are a bigger topic in Data Engineering \\nthat is very significant. \\nFeatures: \\nA well-designed end-to-end data science pipeline can find, collect, manage, analyze, model, and \\ntransform data to uncover possibilities and create cost-effective business operations. \\nCurrent data science pipelines make extracting knowledge from the big dat a you collect simple \\nand quick. \\nThe finest data science pipelines contain the following features to accomplish this: \\n\\uf0b7 Data processing that is both continuous and expandable \\n\\uf0b7 Elasticity and agility afforded by the cloud \\n\\uf0b7 Access to data on a large scale and the capacity to self-serve \\n\\uf0b7 Disaster recovery and high availability \\nMEASURES FOR IN – SAMPLE EVALUATION: \\nMeasures for in – sample evaluation : \\nA way to numerically determine how good the model fits the dataset. \\nTwo important measures to determine the fit of a model: \\n\\uf0b7 Mean squared error(MSE) \\n\\uf0b7 R squared (R^2) \\nMean squared error(MSE) \\n \\nThe Mean Squared Error measures how close a regression line is to a set of data points. It is a risk \\nfunction corresponding to the expected value of the squared error loss.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 83, 'page_label': '84'}, page_content='INTRODUCTION TO DATA SCIENCE \\n84 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nMean square error is calculated by taking the average, specifically the mean, of errors \\nsquared from data as it relates to a function. \\n \\nA larger MSE indicates that the data points are dispersed widely around its central moment \\n(mean), whereas a smaller MSE suggests the opposite. A smaller MSE is preferred because it \\nindicates that your data points are dispersed closely around its central moment (mean). It reflects \\nthe centralized distribution of your data values, the fact that it is not skewed, and, most \\nimportantly, it has fewer errors (errors measured by the dispersion of the data points from its \\nmean). \\nLesser the MSE => Smaller is the error => Better the estimator. \\nThe Mean Squared Error is calculated as: \\nMSE = (1/n) * Σ(actual – forecast)2 \\nwhere: \\n\\uf0b7 Σ – a symbol that means “sum” \\n\\uf0b7 n – sample size \\n\\uf0b7 actual – the actual data value \\n\\uf0b7 forecast – the predicted data value'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 84, 'page_label': '85'}, page_content='INTRODUCTION TO DATA SCIENCE \\n85 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nR squared (R^2) \\nR-squared is a metric of correlation. Correlation is mea sured by “r” and it tells us how strongly \\ntwo variables can be related. A correlation closer to +1 means a strong relationship in the positive \\ndirection, while -1 means a stronger relationship in the opposite direction. A value closer to 0 \\nmeans that there is not much of a relationship between the variables. R -squared is closely related \\nto correlation. \\n \\nThe best way to understand  R-squared is through a simple example. In this example, the black \\nhorizontal line represents the  mean price of houses. The verti cal blue line represents \\nthe variation. Variation means the difference between each point and the mean. The variation of \\nthe data can be calculated by the sum of the squared difference for each point minus the mean. \\nIn other words: Variation = Sum(HousePrice i-Mean)²'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 85, 'page_label': '86'}, page_content='INTRODUCTION TO DATA SCIENCE \\n86 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nUNIT V \\nMODEL EVALUATION \\nGENERALIZATION ERROR \\nEVALUATION METRICS \\nModel Evaluation Metrics define the evaluation metrics for evaluating the performance of a \\nmachine learning model, which is an integral component of any data science project. It aims to \\nestimate the generalization accuracy of a model on the future (unseen/out-of-sample) data. \\n  \\nConfusion Matrix \\n  \\nA confusion matrix is a matrix representation of the prediction results of any binary testing that is \\noften used to describe the performance of the classification model  (or “classifier”) on a set of \\ntest data for which the true values are known. \\nThe confusion matrix itself is relatively simple to understand, but the related terminology can be \\nconfusing.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 86, 'page_label': '87'}, page_content='INTRODUCTION TO DATA SCIENCE \\n87 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nEach prediction can be one of the four outcomes, based on how it matches up to the actual value: \\n\\uf0b7 True Positive (TP): Predicted True and True in reality. \\n\\uf0b7 True Negative (TN): Predicted False and False in reality. \\n\\uf0b7 False Positive (FP): Predicted True and False in reality. \\n\\uf0b7 False Negative (FN): Predicted False and True in reality. \\nNow let us understand this concept using hypothesis testing. \\nA Hypothesis is speculation or theory based on insufficient evidence that lends itself to further \\ntesting and experimentation. With further testing, a hypothesis can usually be proven true or false. \\nA Null Hypothesis is a hypothesis that says there is no statistical significance between the two \\nvariables in the hypothesis. It is the hypothesis that the researcher is trying to disprove. \\nWe would always reject the null hypothesis when it is false, and we would accept the null \\nhypothesis when it is indeed true. \\nEven though hypothesis tests are meant to be reliable, there are two types of errors that can occur.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 87, 'page_label': '88'}, page_content='INTRODUCTION TO DATA SCIENCE \\n88 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nThese errors are known as Type 1 and Type II errors. \\nFor example, when examining the effectiveness of a drug, the null hypothesis would be that the \\ndrug does not affect a disease. \\nType I Error:- equivalent to False Positives(FP). \\nThe first kind of error that is possible involves the rejection of a null hypothesis that is true. \\nLet’s go back to the example of a drug being used to treat a disease. If we reject the null \\nhypothesis in this situation, then we claim that the drug does have some effect on a disease. But if \\nthe null hypothesis is true, then, in reality, the drug does not combat the disease at all. The drug is \\nfalsely claimed to have a positive effect on a disease. \\nType II Error:- equivalent to False Negatives(FN). \\nThe other kind of error that occurs when we accept a false null hypothesis. This sort of  error is \\ncalled a type II error and is also referred to as an error of the second kind. \\nCROSS VALIDATION \\nCross validation is a technique for assessing how the statistical analysis generalises to an \\nindependent data set.It is a technique for evaluating mac hine learning models by training several \\nmodels on subsets of the available input data and evaluating them on the complementary subset of \\nthe data. Using cross-validation, there are high chances that we can detect over-fitting with ease. \\nK-Fold Cross Validation'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 88, 'page_label': '89'}, page_content='INTRODUCTION TO DATA SCIENCE \\n89 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nFirst I would like to introduce you to a golden rule — “Never mix training and test data”. Your \\nfirst step should always be to isolate the test data-set and use it only for final evaluation. Cross-\\nvalidation will thus be performed on the training set. \\n \\nInitially, the entire training data set is broken up in k equal parts. The first part is kept as the hold \\nout (testing) set and the remaining k-1 parts are used to train the model. Then the trained model is \\nthen tested on the holdout set. The above process is repeated k times, in each case we keep on \\nchanging the holdout set. Thus, every data point get an equal opportunity to be included in the test \\nset. \\nUsually, k is equal to 3 or 5. It can be extended even to higher values like 10 or 15 but it becomes \\nextremely computationally expensive and time-consuming. Let us have a look at how we can \\nimplement this with a few lines of Python code and the Sci-kit Learn API. \\n \\nfrom sklearn.model_selection import cross_val_score \\nprint(cross_val_score(model, X_train, y_train, cv=5)) \\nWe pass the model or classifier object, the features, the labels and the parameter cv which \\nindicates the K for K-Fold cross-validation. The method will return a list of k accuracy values for'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 89, 'page_label': '90'}, page_content='INTRODUCTION TO DATA SCIENCE \\n90 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\neach iteration. In general, we take the average of them and use it as a consolidated cross -\\nvalidation score. \\nimport numpy as np \\nprint(np.mean(cross_val_score(model, X_train, y_train, cv=5))) \\nAlthough it might be computationally expensive, cross -validation is essential for \\nevaluating the performance of the learning model. \\nOverfitting and Underfitting : \\nWhat is Overfitting? \\nWhen a model performs very well for training data but has poor performance with test data (new \\ndata), it is known as overfitting. In this case, the machine learning model learns the details and \\nnoise in the training data such that it negatively affects the performance of the model on test data. \\nOverfitting can happen due to low bias and high variance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 90, 'page_label': '91'}, page_content='INTRODUCTION TO DATA SCIENCE \\n91 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nReasons for Overfitting \\n\\uf0b7 Data used for training is not cleaned and contains noise (garbage values) in it \\n\\uf0b7 The model has a high variance \\n\\uf0b7 The size of the training dataset used is not enough \\n\\uf0b7 The model is too complex \\nWays to Tackle Overfitting \\n\\uf0b7 Using K-fold cross-validation \\n\\uf0b7 Using Regularization techniques such as Lasso and Ridge \\n\\uf0b7 Training model with sufficient data \\n\\uf0b7 Adopting ensembling techniques \\nWhat is Underfitting? \\nWhen a model has not learned the patterns in the training data well and is unable to generalize \\nwell on the new data, it is known as underfitting. An underfit model has poor performance on the \\ntraining data and will result in unreliable predictions. Underfitting occurs due to high bias and low \\nvariance.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 91, 'page_label': '92'}, page_content='INTRODUCTION TO DATA SCIENCE \\n92 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nReasons for Underfitting \\n\\uf0b7 Data used for training is not cleaned and contains noise (garbage values) in it \\n\\uf0b7 The model has a high bias \\n\\uf0b7 The size of the training dataset used is not enough \\n\\uf0b7 The model is too simple \\nWays to Tackle Underfitting \\n\\uf0b7 Increase the number of features in the dataset \\n\\uf0b7 Increase model complexity \\n\\uf0b7 Reduce noise in the data \\n\\uf0b7 Increase the duration of training the data \\nRidge Regression'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 92, 'page_label': '93'}, page_content='INTRODUCTION TO DATA SCIENCE \\n93 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nRidge regression is a model tuning method that is used to analyse any data that suffers from \\nmulticollinearity. This method performs L2 regularization. When the issue of multicollinearity \\noccurs, least -squares are unbiased, and variances are large, this results in predicted values \\nbeing far away from the actual values.  \\nThe cost function for ridge regression: \\nMin(||Y – X(theta)||^2 + λ||theta||^2) \\nLambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function.  \\nRidge Regression Models  \\nFor any type of regression machine learning model, the usual regression equation forms the base \\nwhich is written as: \\nY = XB + e \\nWhere Y is the dependent varia ble, X represents the independent variables, B is the regression \\ncoefficients to be estimated, and e represents the errors are residuals.  \\nRidge Regression Predictions \\nWe now show how to make predictions from a Ridge regression model. In particular, we wil l \\nmake predictions based on the Ridge regression model created for Example 1 with lambda = 1.6. \\nThe raw input data is repeated in range A1:E19 of Figure 1 and the unstandardized regression \\ncoefficients calculated in Figure 2 of Ridge Regression Analysis Tool is repeated in range G2:H6 \\nof Figure 1.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 93, 'page_label': '94'}, page_content='INTRODUCTION TO DATA SCIENCE \\n94 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nThe predictions for the input data are shown in column J. In fact, the values in range J2:J19 can be \\ncalculated by the array formula \\n=H2+MMULT(A2:D19,H3:H6). \\nAlternatively, they can be calculated by the array formula \\n=RidgePred(A2:D19,A2:D19,E2:E19,H9) \\nReal Statistics Function: The Real Statistics Resource Pack provides the following functions. \\nRidgeMSE(Rx, Ry, lambda) = MSE of the Ridge regression defined by the x data in Rx, y data in \\nRy and the given lambda value. \\nRidgePred(Rx0, Rx, Ry,  lambda): returns an array of predicted y values for the  x data in range \\nRx0 based on th e Ridge regression model defined by Rx, Ry and  lambda; if Rx0 contains only \\none row then only one y value is returned.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 94, 'page_label': '95'}, page_content='INTRODUCTION TO DATA SCIENCE \\n95 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nGRID SEARCH \\n \\nGrid-search is used to find the optimal  hyperparameters of a model which results in the most \\n‘accurate’ predictions. \\nGrid search refers to a technique used to identify the optimal hyperparameters for a model. Unlike \\nparameters, finding hyperparameters in training data is unattainable. As such, to find the right \\nhyperparameters, we create a model for each combination of hyperparameters. \\nGrid search is thus considered a very traditional hyperparameter optimization method since we are \\nbasically “brute-forcing” all possible combinations. The models are then evaluated through cross-\\nvalidation. The model boasting the best accuracy is naturally considered to be the best. \\n \\n \\nA model hyperparameter is a characteristic of a model that is external to the model and whose \\nvalue cannot be estimated from data. The value of the hyperparameter has to be set before the \\nlearning process begins. For  example, c in Support Vector Machines,  k in k -Nearest \\nNeighbors, the number of hidden layers in Neural Networks.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 95, 'page_label': '96'}, page_content='INTRODUCTION TO DATA SCIENCE \\n96 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nCross validation \\nWe have mentioned that cross -validation is used to evaluate the performance of the models. \\nCross-validation measures how a  model generalizes itself to an independent dataset. We use \\ncross-validation to get a good estimate of how well a predictive model performs. \\nWith this method, we have a pair of datasets: an independent dataset and a training dataset. We \\ncan partition a sin gle dataset to yield the two sets. These partitions are of the same size and are \\nreferred to as folds. A model in consideration is trained on all folds, bar one. \\nThe excluded fold is used to then test the model. This process is repeated until all folds are  used \\nas the test set. The average performance of the model on all folds is then used to estimate the \\nmodel’s performance. \\nIn a technique known as the k -fold cross -validation, a user specifies the number of folds, \\nrepresented by kk. This means that when k=5k=5, there are 5 folds.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 96, 'page_label': '97'}, page_content='INTRODUCTION TO DATA SCIENCE \\n97 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n \\nK-fold cross-validation with K as 5. \\nGrid search implementation \\nThe example given below is a basic implementation of grid search. We first specify the \\nhyperparameters we seek to examine. Then we provide a set of values to test.  \\n1. Load dataset. \\nMy first step is loading the dataset using  from sklearn.datasets import load_iris and iris = \\nload_iris(). The iris dataset is sci -kit learn library in Python. Data is stored in \\na 150∗4150∗4 array.  \\n2. Import GridSearchCV, svm and SVR. \\nAfter loading the dataset, we then import  GridSearchCV as well \\nas svm and SVR from sklearn.model_selection  \\nsklearn.model_selection import GridSearchCV \\nfrom sklearn import svm \\nfrom sklearn.svm import SVR'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 97, 'page_label': '98'}, page_content=\"INTRODUCTION TO DATA SCIENCE \\n98 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n3. Set estimator parameters. \\nIn this implementation, we use the rbf kernel of the  SVR model. rbf stands for the radial basis \\nfunction. It introduces some form of non-linearity to the model since the data in use is non-linear. \\nBy this, we mean that the data arrangement follows no specific sequence. \\nestimator=SVR(kernel='rbf') \\n4. Specify hyperparameters and range of values. \\nWe then specify the hyperparameters we seek to examine. When using the SVR’s  rbf kernel, the \\nthree hyperparameters to use are C, epsilon, and gamma. We can give each one several values to \\nchoose from. \\n5. Evaluation. \\nWe mentioned that cross -validation is carried out to estimate the performance of a model. In k -\\nfold cross-validation, k is the number of folds. As shown below, through  cv=5, we use cross -\\nvalidation to train the model 5 times. This means that 5 would be the kk value. \\n6. Fitting the data. \\nWe do this through grid.fit(X,y), which does the fitting with all the parameters.\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw=extract_text_from_pdf(\"IDS COURSE CONTENT.pdf\")\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"1990s and early 2000s: We can clearly\"\n",
    "results = vector_store.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8687e8f1-87a3-471a-8a69-bc7d9f6288b4', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 1, 'page_label': '2'}, page_content='INTRODUCTION TO DATA SCIENCE \\n2 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nCommunicate: Data Reporting, Data Visualization, Business Intelligence, Decision Making. In \\nthis final step, analysts prepare the analyses in easily readable forms such as charts, graphs, and \\nreports. \\nEvolution of Data Science: Growth & Innovation \\nData science was born from the idea of merging applied statistics with computer science. The \\nresulting field of study would use the extraordinary power of modern computing. Scientists \\nrealized they could not only collect data and solve statistical problems but also use that data to \\nsolve real-world problems and make reliable fact-driven predictions. \\n1962: American mathematician John W. Tukey first articulated the data science dream. In his \\nnow-famous article “The Future of Data Analysis,” he foresaw the inevitable emergence of a new \\nfield nearly two decades before the first personal computers. While Tukey was ahead of his time, \\nhe was not alone in his early appreciation of what would come to be known as “data science.” \\n1977: The theories and predictions of “pre” data scientists like Tukey and Naur became more \\nconcrete with the establishment of The International Association for Statistical Computing \\n(IASC), whose mission was “to link traditional statistical methodology, modern computer \\ntechnology, and the knowledge of domain experts in order to convert data into information and \\nknowledge.” \\n1980s and 1990s: Data science began taking more significant strides with the emergence of the \\nfirst Knowledge Discovery in Databases (KDD) workshop and the founding of the International \\nFederation of Classification Societies (IFCS). \\n1994: Business Week published a story on the new phenomenon of “Database Marketing.” It \\ndescribed the process by which businesses were collecting and leveraging enormous amounts of \\ndata to learn more about their customers, competition, or advertising techniques.'),\n",
       " Document(id='b7ce8683-3674-4b55-b6bb-c1d2d462bf32', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='INTRODUCTION TO DATA SCIENCE \\n1 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\nINTRODUCTION TO DATA SCIENCE \\n LECTURE NOTES \\nUNIT - 1 \\nIntroduction to data science \\nData science: \\nData science is the domain of study that deals with vast volumes of data using modern tools and \\ntechniques to find unseen patterns, derive meaningful information, and make business decisions. \\nData science uses complex machine learning algorithms to build predictive models. \\nThe data used for analysis can come from many different sources and presented in various \\nformats. \\nData science is about extraction, preparation, analysis, visualization, and maintenance of \\ninformation. It is a cross disciplinary field which uses scientific methods and processes to draw \\ninsights from data. \\nThe Data Science Lifecycle \\nData science’s lifecycle consists of five distinct stages, each with its own tasks: \\nCapture: Data Acquisition, Data Entry, Signal Reception, Data Extraction. This stage involves \\ngathering raw structured and unstructured data. \\nMaintain: Data Warehousing, Data Cleansing, Data Staging, Data Processing, Data Architecture. \\nThis stage covers taking the raw data and putting it in a form that can be used. \\nProcess: Data Mining, Clustering/Classification, Data Modeling, Data Summarization. Data \\nscientists take the prepared data and examine its patterns, ranges, and biases to determine how \\nuseful it will be in predictive analysis. \\nAnalyze: Exploratory/Confirmatory, Predictive Analysis, Regression, Text Mining, Qualitative \\nAnalysis. Here is the real meat of the lifecycle. This stage involves performing the various \\nanalyses on the data.'),\n",
       " Document(id='5fa12585-1f6b-46cd-88d9-d99e33a65aa1', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2022-11-25T09:21:07+00:00', 'author': 'NRCM', 'moddate': '2022-11-25T09:21:07+00:00', 'source': 'IDS COURSE CONTENT.pdf', 'total_pages': 98, 'page': 2, 'page_label': '3'}, page_content='INTRODUCTION TO DATA SCIENCE \\n3 \\nCSE NRCM          P.LAKSHMI PRASANNA(ASST.PROFESSOR) \\n1990s and early 2000s: We can clearly see that data science has emerged as a recognized and \\nspecialized field. Several data science academic journals began to circulate, and data science \\nproponents like Jeff Wu and William S. Cleveland continued to help develop and expound upon \\nthe necessity and potential of data science. \\n2000s: Technology made enormous leaps by providing nearly universal access to internet \\nconnectivity, communication, and (of course) data collection. \\n2005: Big data enters the scene. With tech giants such as Google and Facebook uncovering large \\namounts of data, new technologies capable of processing them became necessary. Hadoop rose to \\nthe challenge, and later on Spark and Cassandra made their debuts. \\n2014: Due to the increasing importance of data, and organizations’ interest in finding patterns and \\nmaking better business decisions, demand for data scientists began to see dramatic growth in \\ndifferent parts of the world. \\n2015: Machine learning, deep learning, and Artificial Intelligence (AI) officially enter the realm \\nof data science.  \\n2018: New regulations in the field are perhaps one of the biggest aspects in the evolution in data \\nscience. \\n2020s: We are seeing additional breakthroughs in AI, machine learning, and an ever-more-\\nincreasing demand for qualified professionals in Big Data \\nRoles in Data Science  \\nData Analyst  \\nData Engineers  \\nDatabase Administrator  \\nMachine Learning Engineer')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The role of a data architect in planning and managing end-to-end data architecture involves designing blueprints for data management, ensuring databases are integrated, centralized, and secure. They align the data strategy with business goals, identify appropriate data sources, and collaborate with cross-functional teams to ensure effective database systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import re\n",
    "\n",
    "# Define the prompt template for questions\n",
    "question_prompt_template = \"\"\"\n",
    "You are a helpful assistant that answers questions based on the content of a document. \n",
    "The document's context is as follows:\n",
    "{context}\n",
    "\n",
    "Provide only your final answer without any thinking process or intermediate steps:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt templates\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Initialize the chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': QUESTION_PROMPT},\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "def get_clean_answer(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Get only the final answer from the QA chain response, removing any thinking process.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask\n",
    "        \n",
    "    Returns:\n",
    "        str: The clean answer only\n",
    "    \"\"\"\n",
    "    response = qa_chain.invoke({\"question\": query})\n",
    "    answer = response['answer']\n",
    "    \n",
    "    # Remove thinking process if present\n",
    "    if \"<think>\" in answer.lower():\n",
    "        # Extract everything after the last occurrence of thinking-related content\n",
    "        clean_answer = re.split(r'<think>.*?</think>', answer, flags=re.IGNORECASE|re.DOTALL)[-1]\n",
    "        return clean_answer.strip()\n",
    "    \n",
    "    return answer.strip()\n",
    "\n",
    "# Example usage\n",
    "query = \"planning and managing end-to-end data architecture\"\n",
    "clean_answer = get_clean_answer(query)\n",
    "print(clean_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
